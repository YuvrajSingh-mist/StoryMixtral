{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#Transformer version of TacoTron\n",
        "!pip install nltk\n",
        "import nltk\n",
        "nltk.download('cmudict')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I9wM8HrXVx5A",
        "outputId": "4de94005-9dbf-4630-efcf-e91f1b3d3c1c"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]   Package cmudict is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "_cell_guid": "e4b2df23-1b2f-4ca2-9968-9bbd2972779d",
        "_uuid": "4b54e35b-02bb-44b4-adca-a297c3081d36",
        "execution": {
          "iopub.execute_input": "2025-03-16T20:27:50.949322Z",
          "iopub.status.busy": "2025-03-16T20:27:50.949124Z",
          "iopub.status.idle": "2025-03-16T20:27:53.952701Z",
          "shell.execute_reply": "2025-03-16T20:27:53.951790Z",
          "shell.execute_reply.started": "2025-03-16T20:27:50.949302Z"
        },
        "id": "Pw7f2ghccuoK",
        "jupyter": {
          "outputs_hidden": false
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from pathlib import Path\n",
        "import random\n",
        "\n",
        "\n",
        "# from tokenizers import Tokenizer\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from nltk.corpus import cmudict\n",
        "pronouncing_dict = cmudict.dict()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word = \"hello\"\n",
        "phonemes = pronouncing_dict.get(word.lower(), [\"Not found\"])\n",
        "print(phonemes)  # Output: [['HH', 'AH0', 'L', 'OW1']]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bYzk5LW-vwHD",
        "outputId": "47979391-f958-483d-b2ba-398b62b00edf"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['HH', 'AH0', 'L', 'OW1'], ['HH', 'EH0', 'L', 'OW1']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# text_to_phonemes(\"hello\")"
      ],
      "metadata": {
        "id": "9PbhTw-4rHuD"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# temp = text_to_phonemes(\"hello\")\n",
        "# temp2 = text_to_phonemes(\"world\")\n",
        "# temp.extend(temp2)\n",
        "# # temp\n",
        "# phonemes_to_indices(temp)"
      ],
      "metadata": {
        "id": "RmQNVnpCr4o4"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def text_to_phonemes(word):\n",
        "    \"\"\"Convert a word to its phoneme sequence using CMUdict.\"\"\"\n",
        "    word = word.lower()\n",
        "    if word in pronouncing_dict:\n",
        "        return pronouncing_dict[word][0]  # Take the first pronunciation if multiple exist\n",
        "    else:\n",
        "        return None  # Handle OOV words separately\n",
        "phoneme_vocab = set()\n",
        "for word, phonemes in pronouncing_dict.items():\n",
        "    for phoneme_seq in phonemes:\n",
        "        phoneme_vocab.update(phoneme_seq)\n",
        "\n",
        "phoneme_vocab = sorted(list(phoneme_vocab))  # Sort for consistent indexing\n",
        "phoneme_vocab.insert(0, '[PAD]')\n",
        "phoneme_vocab.insert(1, '[EOS]')\n",
        "phoneme_vocab.insert(2, '[UNK]')\n",
        "phoneme_to_id = {phoneme: idx for idx, phoneme in enumerate(phoneme_vocab)}\n",
        "\n",
        "vocab_size = len(phoneme_vocab)  # Number of unique phonemes\n",
        "\n",
        "def phonemes_to_indices(phoneme_seq):\n",
        "    \"\"\"Convert a list of phonemes to their corresponding indices.\"\"\"\n",
        "    return [phoneme_to_id[p] for p in phoneme_seq if p in phoneme_to_id]\n"
      ],
      "metadata": {
        "id": "4Qbwuf2bL_xK"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MwblB-OhNUJH",
        "outputId": "490b659e-f087-4d58-9efd-7c759d99169f"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "73"
            ]
          },
          "metadata": {},
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "_cell_guid": "8e81d8cb-2c43-4550-a8f8-09815ff2a1ab",
        "_uuid": "fe2c17e3-b1bf-4b93-8760-de26e849e84f",
        "execution": {
          "iopub.execute_input": "2025-03-16T20:27:53.954751Z",
          "iopub.status.busy": "2025-03-16T20:27:53.954400Z",
          "iopub.status.idle": "2025-03-16T20:27:56.417272Z",
          "shell.execute_reply": "2025-03-16T20:27:56.416335Z",
          "shell.execute_reply.started": "2025-03-16T20:27:53.954727Z"
        },
        "id": "LwR5_uvTcuoL",
        "jupyter": {
          "outputs_hidden": false
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "\n",
        "# import re\n",
        "HF_TOKEN = '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'\n",
        "\n",
        "# from transformers import AutoTokenizer\n",
        "\n",
        "# tokenizer = AutoTokenizer.from_pretrained(\"openai-community/gpt2\", token=HF_TOKEN)\n",
        "# tokenizer.add_special_tokens({\"pad_token\": \"[PAD]\"})\n",
        "\n",
        "# SOT = '<|startoftranscript|>'\n",
        "# EOT = '<|endoftranscript|>'\n",
        "# transcribe = '<|transcribe|>'\n",
        "# prev = '<|prev|>'\n",
        "\n",
        "# special_tokens_dict = {\n",
        "#     'additional_special_tokens': [SOT, EOT, transcribe, prev]\n",
        "# }\n",
        "\n",
        "\n",
        "# tokenizer.add_special_tokens(special_tokens_dict)\n",
        "# model = AutoModelForCausalLM.from_pretrained(\"openai-community/gpt2\")\n",
        "\n",
        "# tokenizer(\"hi\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "!wandb login"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_UrQBMMqqHMV",
        "outputId": "0a3e06de-5124-443f-e322-07eae8d5d95f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mrajceo2031\u001b[0m (\u001b[33mrentio\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "_cell_guid": "32170d73-d4d4-431e-ad94-fa5d9addd61f",
        "_uuid": "234d59ef-e07b-4028-a613-ec9e72abfd67",
        "execution": {
          "iopub.execute_input": "2025-03-16T20:27:56.419235Z",
          "iopub.status.busy": "2025-03-16T20:27:56.418752Z",
          "iopub.status.idle": "2025-03-16T20:28:08.001610Z",
          "shell.execute_reply": "2025-03-16T20:28:08.000672Z",
          "shell.execute_reply.started": "2025-03-16T20:27:56.419209Z"
        },
        "id": "P3vCVc6OlXe2",
        "jupyter": {
          "outputs_hidden": false
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "\n",
        "# !pip install wandb\n",
        "# import wandb\n",
        "# from kaggle_secrets import UserSecretsClient\n",
        "# user_secrets = UserSecretsClient()\n",
        "# secret_value_0 = user_secrets.get_secret(\"API_KEY\")\n",
        "\n",
        "# wandb.login(key=secret_value_0)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Hyperparameters\n",
        "epochs=10\n",
        "block_size = 64\n",
        "batch_size = 64\n",
        "# src_vocab_size = None\n",
        "src_vocab_size = vocab_size\n",
        "phenome_embeddings_dims = 512\n",
        "embeddings_dims = phenome_embeddings_dims\n",
        "prenet_encoder_embeddings_dims = 512\n",
        "attn_dropout = 0.1\n",
        "no_of_heads = 4 #IMP needs to be thoroughly calculated\n",
        "dropout = 0.1\n",
        "# epochs = 3\n",
        "max_lr = 6e-4\n",
        "no_of_decoder_layers = 8 #IMP needs to be thoroughly calculated\n",
        "attn_dropout = 0.1\n",
        "weight_decay_optim = 0.01\n",
        "log_mel_features = 80\n",
        "kernel_size = (5,1)\n",
        "stride = (2,10)\n",
        "sr = 16000\n",
        "device= 'cpu'\n",
        "SAMPLING_RATE=16000\n",
        "N_MELS = 80  # 80-channel Mel spectrogram\n",
        "WINDOW_DURATION = 0.050  # 25 milliseconds\n",
        "STRIDE_DURATION = 0.0125  # 10 milliseconds\n",
        "max_t = 500\n",
        "n_channels = N_MELS\n",
        "clip = 1.0\n",
        "embeddings_dims_decoder = 256"
      ],
      "metadata": {
        "id": "D7AP219KJzTs"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "5ac94789-1d5e-4ace-88f3-25b5413fb78b",
        "_uuid": "92a80d68-b0eb-4f1d-b988-7f190e0b934a",
        "execution": {
          "iopub.execute_input": "2025-03-16T20:28:08.020631Z",
          "iopub.status.busy": "2025-03-16T20:28:08.020054Z",
          "iopub.status.idle": "2025-03-16T20:28:08.043556Z",
          "shell.execute_reply": "2025-03-16T20:28:08.042503Z",
          "shell.execute_reply.started": "2025-03-16T20:28:08.020565Z"
        },
        "id": "TmPkI_UEpvor",
        "jupyter": {
          "outputs_hidden": false
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "torch.set_default_device(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "_cell_guid": "1777fa9a-7686-4b62-93c1-c31c2ccf73c1",
        "_uuid": "e08f94e6-dae5-42f3-a285-c151086b8f1b",
        "execution": {
          "iopub.execute_input": "2025-03-16T20:28:08.045189Z",
          "iopub.status.busy": "2025-03-16T20:28:08.044805Z",
          "iopub.status.idle": "2025-03-16T20:28:14.373089Z",
          "shell.execute_reply": "2025-03-16T20:28:14.372056Z",
          "shell.execute_reply.started": "2025-03-16T20:28:08.045149Z"
        },
        "id": "IME1Ls95Y3gl",
        "jupyter": {
          "outputs_hidden": false
        },
        "outputId": "848ac0b7-5ab3-42ce-be83-7a25e21622bb",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.4.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.12.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.14)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.29.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.2.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['id', 'audio', 'file', 'text', 'normalized_text'],\n",
            "        num_rows: 13100\n",
            "    })\n",
            "})\n"
          ]
        }
      ],
      "source": [
        "\n",
        "!pip install datasets\n",
        "from tabnanny import verbose\n",
        "from datasets import load_dataset\n",
        "\n",
        "gs = load_dataset(\"keithito/lj_speech\", token=HF_TOKEN)\n",
        "\n",
        "\n",
        "print(gs)\n",
        "\n",
        "\n",
        "audio_input = gs['train'][0][\"audio\"]\n",
        "transcription = gs[\"train\"][0][\"text\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "_cell_guid": "0c018b51-6161-49d9-995e-21cf09d1391f",
        "_uuid": "027b3fe3-8e01-40c4-99de-1a1b12d93453",
        "execution": {
          "iopub.execute_input": "2025-03-16T20:28:14.374940Z",
          "iopub.status.busy": "2025-03-16T20:28:14.374239Z",
          "iopub.status.idle": "2025-03-16T20:35:37.108931Z",
          "shell.execute_reply": "2025-03-16T20:35:37.108126Z",
          "shell.execute_reply.started": "2025-03-16T20:28:14.374907Z"
        },
        "id": "cRV1EOlVY3gm",
        "jupyter": {
          "outputs_hidden": false
        },
        "outputId": "94f5cd89-5a13-4aa1-a390-b9ffe5ffce47",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 182,
          "referenced_widgets": [
            "f582fde0045543b9826c2a5808e1836c",
            "ac1fd3699fe84a47aedb002bc285f094",
            "4a238f8102e54ea6bde07382ead69adf",
            "54b5753483a04ace97454b852d422686",
            "e9506522e1a447bbaca31e38ec791f05",
            "6fd54e53840045a497406dcafdb386b0",
            "e2cc4cb317334388ab6f4fb0b17bcbe3",
            "561c0da5a1184145995c8746ca39f565",
            "6e148d7d730340b5b5e1dcac1c843237",
            "443e9f22bd2b448a8e269e63983f13d9",
            "00d8358718b34a3e949a916d553295db",
            "123dc6f1d5234259a441a65ec8051463",
            "add9244242d3438bb0c55999c7f398bd",
            "ca83532cd81e4d319d4db258f939f0d8",
            "51a35a70be0d480a816828b682f40589",
            "aa5e87f2aa3844b597da58e2a0f24e81",
            "004db210db4d456890ecfdf317ef02f5",
            "9fbe078526744495bdc1324948e7a0ce",
            "250c4728407044faa0f17204d854ebf7",
            "e53ffc1619a046e7ae08393f4a6b41c2",
            "d3dfa012744c46fbb325e98d343e937d",
            "331478bcce864bf79dd0536027508930",
            "1922829153554bcaa5ce5a6800c2d275",
            "0ea10580caa04c949349ec5009d48b30",
            "927489fc0f5a4513ad02856f695713dd",
            "e70a1a9c1c0442e4bd942cfa34af30e4",
            "48651b3ffd73444791f8133e2e26220c",
            "74adcac4afce41fdb3942f851ca6d76d",
            "1f09efeed3634f62918cd0494ce57e01",
            "cf538f36c9b149d3b2bacd43a920aae2",
            "dd100a076a5f4f2baa9cbd2db375d49e",
            "e67ba48851b04f7ea8796732c54fffb2",
            "9dacc8519823484eaa91f7cbd1bc4f1a",
            "c7b0af32d3e04560a8f0599ab80cd03b",
            "4a3c91f68a4144e3874a847532edcd97",
            "8444e340e21142da8feb5e494f9e3574",
            "72094db159df435397f941d6ed6f8f99",
            "e36e1c1b23bf46d7b027048f2f5a2d59",
            "952c5a6d7d084337b73d94ce620672cd",
            "d311901336cb461cadca3ae7bb04537c",
            "adfcda976ab54a8bace5657809a2451d",
            "d65629f2014049f3a063d611f7994cc7",
            "0ceeb78ae5154fd58f1a2dd7d17d072b",
            "be9fc651c297414fb4e2a3317ea89f8f"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10480/10480 [01:05<00:00, 161.12it/s]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Flattening the indices:   0%|          | 0/10480 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f582fde0045543b9826c2a5808e1836c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Filter:   0%|          | 0/10480 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "123dc6f1d5234259a441a65ec8051463"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2620/2620 [00:11<00:00, 221.36it/s]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Flattening the indices:   0%|          | 0/2620 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1922829153554bcaa5ce5a6800c2d275"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Filter:   0%|          | 0/2620 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c7b0af32d3e04560a8f0599ab80cd03b"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "\n",
        "MAX_DURATION_IN_SECONDS = 10\n",
        "\n",
        "gs = gs['train'].train_test_split(test_size=0.2)\n",
        "# print(dataset)\n",
        "# train_data, val_data = dataset['train'], dataset['test']\n",
        "\n",
        "import librosa\n",
        "from tqdm import tqdm\n",
        "def is_audio_length_in_range(input_length):\n",
        "    return input_length < MAX_DURATION_IN_SECONDS\n",
        "\n",
        "train_new_column = []\n",
        "\n",
        "for x in tqdm(range(len(gs['train']))):\n",
        "    train_new_column.append(librosa.get_duration(path=gs['train'][x]['audio']['path']))\n",
        "\n",
        "gs_ = gs['train'].add_column(\"duration\", train_new_column)\n",
        "\n",
        "\n",
        "gs_ = gs_.filter(is_audio_length_in_range, input_columns=[\"duration\"])\n",
        "\n",
        "\n",
        "truncated_gs_train = gs_.remove_columns([\"duration\"])\n",
        "# truncated_gs\n",
        "\n",
        "\n",
        "\n",
        "val_new_column = []\n",
        "# new_column = [librosa.get_duration(path=x) ]]\n",
        "for x in tqdm(range(len(gs['test']))):\n",
        "    val_new_column.append(librosa.get_duration(path=gs['test'][x]['audio']['path']))\n",
        "\n",
        "gs_ = gs['test'].add_column(\"duration\", val_new_column)\n",
        "\n",
        "\n",
        "gs_ = gs_.filter(is_audio_length_in_range, input_columns=[\"duration\"])\n",
        "\n",
        "\n",
        "truncated_gs_val = gs_.remove_columns([\"duration\"])\n",
        "# truncated_gs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "_cell_guid": "68c3295a-6eef-4874-acaf-73f9279fad98",
        "_uuid": "d988499f-ad5b-47cf-bd9e-257395584889",
        "execution": {
          "iopub.execute_input": "2025-03-16T20:35:37.111778Z",
          "iopub.status.busy": "2025-03-16T20:35:37.111522Z",
          "iopub.status.idle": "2025-03-16T21:17:23.216508Z",
          "shell.execute_reply": "2025-03-16T21:17:23.215522Z",
          "shell.execute_reply.started": "2025-03-16T20:35:37.111754Z"
        },
        "id": "6NZ9Hbp5q1to",
        "jupyter": {
          "outputs_hidden": false
        },
        "outputId": "8b9f8de9-eaea-4881-cddd-9746354e64f0",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10323/10323 [05:49<00:00, 29.51it/s]\n",
            "100%|██████████| 2587/2587 [01:20<00:00, 32.06it/s]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "n_fft = int(WINDOW_DURATION * MAX_DURATION_IN_SECONDS * SAMPLING_RATE)\n",
        "hop_length = int(STRIDE_DURATION * MAX_DURATION_IN_SECONDS * SAMPLING_RATE)\n",
        "\n",
        "train_outputs = []\n",
        "train_texts = []\n",
        "for i in tqdm(range(len(truncated_gs_train))):\n",
        "  S = librosa.feature.melspectrogram(\n",
        "      y=truncated_gs_train[i]['audio']['array'],\n",
        "      sr=SAMPLING_RATE,\n",
        "      n_mels=N_MELS,\n",
        "      n_fft=n_fft,\n",
        "      hop_length=hop_length,\n",
        "      win_length=n_fft,\n",
        "      fmax=SAMPLING_RATE // 2\n",
        "  )\n",
        "\n",
        "\n",
        "  S_dB = librosa.power_to_db(S, ref=np.max)\n",
        "  train_outputs.append(S_dB)\n",
        "  train_texts.append(truncated_gs_train[i]['text'])\n",
        "\n",
        "val_outputs = []\n",
        "val_texts = []\n",
        "for i in tqdm(range(len(truncated_gs_val))):\n",
        "  S = librosa.feature.melspectrogram(\n",
        "      y=truncated_gs_val[i]['audio']['array'],\n",
        "      sr=SAMPLING_RATE,\n",
        "      n_mels=N_MELS,\n",
        "      n_fft=n_fft,\n",
        "      hop_length=hop_length,\n",
        "      win_length=n_fft,\n",
        "      fmax=SAMPLING_RATE // 2\n",
        "  )\n",
        "\n",
        "\n",
        "  S_dB = librosa.power_to_db(S, ref=np.max)\n",
        "  val_outputs.append(S_dB)\n",
        "  val_texts.append(truncated_gs_val[i]['text'])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_outputs[0].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oXcdjXzEenwE",
        "outputId": "1f4515cb-e94e-4fd9-9d04-1a42cebf73fe"
      },
      "execution_count": 148,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(80, 71)"
            ]
          },
          "metadata": {},
          "execution_count": 148
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 162,
      "metadata": {
        "_cell_guid": "db1f5e13-91c6-4d24-8d31-325796a8a0fd",
        "_uuid": "b456c1a6-6703-4809-a1fb-fc077a5dc8e3",
        "execution": {
          "iopub.execute_input": "2025-03-16T21:17:23.219755Z",
          "iopub.status.busy": "2025-03-16T21:17:23.219057Z",
          "iopub.status.idle": "2025-03-16T21:17:23.229394Z",
          "shell.execute_reply": "2025-03-16T21:17:23.228433Z",
          "shell.execute_reply.started": "2025-03-16T21:17:23.219721Z"
        },
        "id": "z2aGf6_7xe9S",
        "jupyter": {
          "outputs_hidden": false
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# import math\n",
        "import re\n",
        "# print(round(random.random(), 1))\n",
        "class TTSDataset(Dataset):\n",
        "\n",
        "  def __init__(self, outputs, texts):\n",
        "\n",
        "    self.data = outputs\n",
        "    self.texts = texts\n",
        "    self.max_t = block_size\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.data)\n",
        "\n",
        "\n",
        "  def pad_phoneme_sequence(self, phoneme_seq, max_length):\n",
        "        \"\"\"Pads phoneme sequences to max_length.\"\"\"\n",
        "        pad_token = 0\n",
        "        if len(phoneme_seq) < max_length:\n",
        "            phoneme_seq += [pad_token] * (max_length - len(phoneme_seq))\n",
        "        else:\n",
        "            phoneme_seq = phoneme_seq[:max_length]\n",
        "        return phoneme_seq\n",
        "\n",
        "\n",
        "  def pad_to_max_t(self, spectrogram, max_t):\n",
        "\n",
        "    n_mels, t = spectrogram.shape\n",
        "    if t < max_t:\n",
        "        # Pad with zeros\n",
        "        pad_width = ((0, 0), (0, max_t - t))\n",
        "        spectrogram = np.pad(spectrogram, pad_width, mode='constant')\n",
        "    else:\n",
        "      spectrogram = spectrogram[:, :max_t]\n",
        "\n",
        "    return spectrogram\n",
        "\n",
        "  def clean(self, desc):\n",
        "    # Use regex to remove anything between < and >\n",
        "    cleaned_text = re.sub(r'<[^>]*>', '', desc)\n",
        "    return cleaned_text\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "\n",
        "      # SOT = '<|startoftranscript|>'\n",
        "      # EOT = '<|endoftranscript|>'\n",
        "      # transcribe = '<|transcribe|>'\n",
        "      # prev = '<|prev|>'\n",
        "      spectrogram = self.pad_to_max_t(self.data[idx], self.max_t)\n",
        "      # probs = round(random.random(),1)\n",
        "      spectrogram = torch.tensor(spectrogram, dtype=torch.float32)\n",
        "\n",
        "      # if(probs == 0.5):\n",
        "        # Normalize the spectrogram between -1 and 1\n",
        "      spectrogram_min = spectrogram.min()\n",
        "      spectrogram_max = spectrogram.max()\n",
        "      # spectrogram = spectrogram.unsqueeze(0)  # Shape: (1, n_mels, max_t)\n",
        "      # prev_text =\n",
        "      # text = self.clean(self.texts[idx])\n",
        "      text = self.texts[idx]\n",
        "      text = text.lower()\n",
        "      # text = SOT  + 'en' + transcribe +  text + EOT\n",
        "      # text += '[EOS]'\n",
        "      # tokenized_text = tokenizer(text, truncation=True, padding='max_length', max_length=block_size, return_tensors='pt')\n",
        "      text = text.split(' ')\n",
        "      phenomes = []\n",
        "      # print(text)\n",
        "      temp = []\n",
        "      # tokenized_text = []\n",
        "      tokenized_text = {}\n",
        "      # for batch in range(batch_size):\n",
        "      for i in range(len(text)):\n",
        "        phenomes_now = text_to_phonemes(text[i])\n",
        "        # print(phenomes_now)\n",
        "        if(phenomes_now == None):\n",
        "          temp = phonemes_to_indices('[UNK]')\n",
        "        else:\n",
        "          temp = phonemes_to_indices(phenomes_now)\n",
        "        phenomes.extend(temp)\n",
        "      #   tokenized_text.append(phenomes)\n",
        "      #   temp = []\n",
        "      #   phenomes = []\n",
        "\n",
        "      # tokenized_text = torch.stack([tokenized_text])\n",
        "      # print(text)\n",
        "\n",
        "      # print(phenomes)\n",
        "      phenomes = self.pad_phoneme_sequence(phenomes, block_size)\n",
        "      # print(phenomes)\n",
        "      tokenized = torch.tensor(phenomes, dtype=torch.long)\n",
        "      tokenized_text['input_ids'] = tokenized\n",
        "      # print(tokenized_text.shape)\n",
        "\n",
        "      epsilon = 1e-8  # To avoid division by zero\n",
        "      spectrogram = 2 * ((spectrogram - spectrogram_min) / (spectrogram_max - spectrogram_min + epsilon)) - 1\n",
        "\n",
        "      # tokenized_win_prompt = tokenizer(text, max_length = ModelArgs.block_size, padding='max_length', truncation=True,  return_tensors=\"pt\").to(device)\n",
        "      tokenized_text['labels'] = tokenized_text['input_ids'].clone()\n",
        "      # tokenized_text['labels'][:-1] = tokenized_text['input_ids'][: , 1:]\n",
        "      # tokenized_text['labels'][: , -1] = phonemes_to_indices('[EOS]')\n",
        "\n",
        "      # tokenized_text_x = tokenized_text['input_ids'].squeeze(0)\n",
        "      # tokenized_text_y = tokenized_text['labels'].squeeze(0)\n",
        "\n",
        "      # print(tokenized_text.shape)\n",
        "      # print(\"dataset: \", tokenized_text)\n",
        "      return spectrogram, tokenized_text"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def collate_fn(batch):\n",
        "    spectrograms = []\n",
        "    input_ids_list = []\n",
        "    labels_list = []\n",
        "\n",
        "    for spec, text_dict in batch:\n",
        "        # 1. Collect spectrograms\n",
        "        spectrograms.append(spec)\n",
        "\n",
        "        # 2. Extract text components\n",
        "        input_ids_list.append(text_dict['input_ids'])\n",
        "        labels_list.append(text_dict['labels'])\n",
        "\n",
        "    # 3. Stack tensors\n",
        "    spectrograms = torch.stack(spectrograms)\n",
        "    input_ids = torch.stack(input_ids_list)\n",
        "    labels = torch.stack(labels_list)\n",
        "\n",
        "    # 4. Return in proper format\n",
        "    return {\n",
        "        'spectrograms': spectrograms,\n",
        "        'input_ids': input_ids,\n",
        "        'labels': labels\n",
        "    }\n"
      ],
      "metadata": {
        "id": "BWiwc8a5Qkhe"
      },
      "execution_count": 159,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = TTSDataset(train_outputs, train_texts)"
      ],
      "metadata": {
        "id": "LAwuExYnNkHq"
      },
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for x,y,z in dataset:\n",
        "  print(x)\n",
        "  print(y)\n",
        "  print(z)\n",
        "  break"
      ],
      "metadata": {
        "id": "JTvcffANNpnc"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {
        "_cell_guid": "78e27c8d-e3cd-434c-b2c6-9d16b79eb5be",
        "_uuid": "5ca81223-5c7e-43e5-83b2-7b00a3ecee7f",
        "execution": {
          "iopub.execute_input": "2025-03-16T21:17:23.230758Z",
          "iopub.status.busy": "2025-03-16T21:17:23.230437Z",
          "iopub.status.idle": "2025-03-16T21:17:23.258809Z",
          "shell.execute_reply": "2025-03-16T21:17:23.257804Z",
          "shell.execute_reply.started": "2025-03-16T21:17:23.230731Z"
        },
        "jupyter": {
          "outputs_hidden": false
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2FJvDV-4psOo",
        "outputId": "b5512f27-b3b2-4fb3-be04-6048861190b8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch.autograd.anomaly_mode.set_detect_anomaly at 0x78437ef34650>"
            ]
          },
          "metadata": {},
          "execution_count": 98
        }
      ],
      "source": [
        "\n",
        "torch.autograd.set_detect_anomaly(True)  # Add at the start of training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 163,
      "metadata": {
        "_cell_guid": "d26cf6eb-55b2-484c-bc9b-10952ea5c977",
        "_uuid": "8db84c70-d655-4b7c-8c00-9e195d240a7d",
        "execution": {
          "iopub.execute_input": "2025-03-16T21:17:23.260278Z",
          "iopub.status.busy": "2025-03-16T21:17:23.259908Z",
          "iopub.status.idle": "2025-03-16T21:17:23.478760Z",
          "shell.execute_reply": "2025-03-16T21:17:23.477736Z",
          "shell.execute_reply.started": "2025-03-16T21:17:23.260242Z"
        },
        "id": "2tvMuBSOynPy",
        "jupyter": {
          "outputs_hidden": false
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "shuffle = True\n",
        "\n",
        "train_dataset = TTSDataset(train_outputs, train_texts)\n",
        "val_dataset = TTSDataset(val_outputs, val_texts)\n",
        "\n",
        "# generator = torch.Generator(device=device)\n",
        "\n",
        "train_dataloader = DataLoader(\n",
        "\n",
        "    train_dataset,\n",
        "    batch_size=batch_size,\n",
        "    # generator=generator,\n",
        "    shuffle=shuffle,\n",
        "     drop_last=True,\n",
        "    collate_fn=collate_fn\n",
        ")\n",
        "val_dataloader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=batch_size,\n",
        "\n",
        "    # generator=generator,\n",
        "    drop_last=True ,\n",
        "    shuffle=False,\n",
        "    collate_fn = collate_fn\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 165,
      "metadata": {
        "_cell_guid": "a7c6dcaa-5a5d-4260-8696-ee827274edff",
        "_uuid": "25ee96a4-00b2-40a5-a8e4-10ea78a00481",
        "execution": {
          "iopub.execute_input": "2025-03-16T21:17:25.575666Z",
          "iopub.status.busy": "2025-03-16T21:17:25.575422Z",
          "iopub.status.idle": "2025-03-16T21:17:25.580222Z",
          "shell.execute_reply": "2025-03-16T21:17:25.579208Z",
          "shell.execute_reply.started": "2025-03-16T21:17:25.575643Z"
        },
        "id": "kzXk76ULY3gm",
        "jupyter": {
          "outputs_hidden": false
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "#Position embeddings\n",
        "class PositionEmbeddings(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        embeddings_dims = embeddings_dims,\n",
        "        block_size = block_size\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.position_embeddings = nn.Parameter(torch.randn(1, block_size, embeddings_dims, device=device), requires_grad=True) #To give positional embeddings to each token of the input text, hence num_embeddings=block_size\n",
        "        # nn.init.normal_(self.position_embeddings.weight.data, mean=0, std=0.02)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.position_embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "779e1bc7-798e-4e86-ad9e-0967b08dcab9",
        "_uuid": "6c495fe4-d351-4477-be75-b575ea534db0",
        "execution": {
          "iopub.execute_input": "2025-03-16T21:17:25.581153Z",
          "iopub.status.busy": "2025-03-16T21:17:25.580958Z",
          "iopub.status.idle": "2025-03-16T21:17:25.631084Z",
          "shell.execute_reply": "2025-03-16T21:17:25.630436Z",
          "shell.execute_reply.started": "2025-03-16T21:17:25.581135Z"
        },
        "jupyter": {
          "outputs_hidden": false
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-p1sURm3psOp",
        "outputId": "9d078a5b-52ee-4675-c0e6-99d14e5db292"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Parameter containing:\n",
              "tensor([[[ 0.4668,  0.4255,  0.0655,  ...,  1.0546,  1.4810, -0.2844],\n",
              "         [-0.7837, -0.4969, -0.2619,  ..., -0.3950,  1.0616,  0.1200],\n",
              "         [ 0.6163,  0.3693,  0.9264,  ..., -0.4270,  0.8463, -0.3498],\n",
              "         ...,\n",
              "         [ 0.4406, -0.2102,  0.6724,  ...,  1.1604,  0.4365, -0.7942],\n",
              "         [-0.5134,  0.4422,  1.1975,  ..., -0.0677, -0.3310, -0.5226],\n",
              "         [ 0.6912,  1.1274,  1.0454,  ..., -0.9177, -1.9010,  0.2635]]],\n",
              "       device='cuda:0', requires_grad=True)"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "pos = PositionEmbeddings()\n",
        "x = torch.randn(batch_size, block_size, embeddings_dims)\n",
        "pos(x)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class PrenetEncoder(nn.Module):\n",
        "  def __init__(\n",
        "      self,\n",
        "\n",
        "  ):\n",
        "    super().__init__()\n",
        "    self.device = device\n",
        "    self.embeds_dims = phenome_embeddings_dims\n",
        "    self.out = prenet_encoder_embeddings_dims\n",
        "    self.conv1d_layer1 = nn.Conv1d(in_channels=self.embeds_dims, out_channels=self.out, kernel_size=kernel_size, device=self.device, padding=1)\n",
        "    self.conv1d_layer2 = nn.Conv1d(in_channels=self.out, out_channels=self.out, kernel_size=kernel_size, device=self.device, padding=1)\n",
        "    self.conv1d_layer3 = nn.Conv1d(in_channels=self.out, out_channels=self.out, kernel_size=kernel_size, device=self.device, padding=1)\n",
        "    self.norm = torch.nn.BatchNorm1d(self.out)\n",
        "    self.proj = nn.Linear(self.out, self.out, device=self.device)\n",
        "  def forward(self, x):\n",
        "\n",
        "    x = self.conv1d_layer1(x)\n",
        "    x = self.norm(x)\n",
        "    x = torch.nn.functional.relu(x)\n",
        "    x = self.conv1d_layer2(x)\n",
        "    x = self.norm(x)\n",
        "    x = torch.nn.functional.relu(x)\n",
        "    # x = torch.nn.functional.gelu(x)\n",
        "    x = self.conv1d_layer3(x)\n",
        "    x = self.norm(x)\n",
        "    x = torch.nn.functional.relu(x)\n",
        "    x = self.proj(x)\n",
        "    # print(x.shape)\n",
        "    return x\n"
      ],
      "metadata": {
        "id": "0slFOCCu7gYz"
      },
      "execution_count": 166,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PrenetDecoder(nn.Module):\n",
        "\n",
        "  def __init__(\n",
        "      self,\n",
        "\n",
        "  ):\n",
        "    super().__init__()\n",
        "    self.decoder_embeds_dims = embeddings_dims_decoder\n",
        "    self.device = device\n",
        "    self.out = phenome_embeddings_dims\n",
        "    self.linear_layer1 = nn.Linear(in_features=N_MELS, out_features=self.decoder_embeds_dims, device=self.device)\n",
        "    self.linear_layer2 = nn.Linear(in_features=self.decoder_embeds_dims, out_features=self.out, device=self.device)\n",
        "    self.linear_layer3 = nn.Linear(self.out, self.out, device=self.device)\n",
        "  def forward(self, x):\n",
        "    x = self.linear_layer1(x)\n",
        "    x = torch.nn.functional.relu(x)\n",
        "    x = self.linear_layer2(x)\n",
        "    x = torch.nn.functional.relu(x)\n",
        "    x = self.linear_layer3(X)\n",
        "    return x\n"
      ],
      "metadata": {
        "id": "CWSKPVpN0hZg"
      },
      "execution_count": 167,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 168,
      "metadata": {
        "_cell_guid": "62818f89-119f-4059-8538-2feefb03e6a7",
        "_uuid": "8c520ea7-c368-4ce4-914c-2094cb5ed848",
        "execution": {
          "iopub.execute_input": "2025-03-16T21:17:25.631941Z",
          "iopub.status.busy": "2025-03-16T21:17:25.631754Z",
          "iopub.status.idle": "2025-03-16T21:17:25.636182Z",
          "shell.execute_reply": "2025-03-16T21:17:25.635330Z",
          "shell.execute_reply.started": "2025-03-16T21:17:25.631924Z"
        },
        "id": "R9CSiuD2jHyT",
        "jupyter": {
          "outputs_hidden": false
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# # Mel spectograms embeddings\n",
        "# class TgtMelEmbeddings(nn.Module):\n",
        "#     def __init__(\n",
        "#         self,\n",
        "#         vocab_size = tgt_vocab_size,\n",
        "#         embeddings_dims = embeddings_dims\n",
        "#     ):\n",
        "#         super().__init__()\n",
        "#         self.embeddings_table = nn.Embedding(num_embeddings = tgt_vocab_size, embedding_dim=embeddings_dims, device=device) #Just a look up table to convert the toekns_ids to some numbers\n",
        "#         # nn.init.normal_(self.embeddings_table.weight.data, mean=0, std=0.02)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         return self.embeddings_table(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 169,
      "metadata": {
        "_cell_guid": "0e795d31-2801-446a-807d-0ea3bebb6e3d",
        "_uuid": "ae3226bb-4414-4840-8da7-73db14ad8271",
        "execution": {
          "iopub.execute_input": "2025-03-16T21:17:25.637345Z",
          "iopub.status.busy": "2025-03-16T21:17:25.637118Z",
          "iopub.status.idle": "2025-03-16T21:17:25.653427Z",
          "shell.execute_reply": "2025-03-16T21:17:25.652838Z",
          "shell.execute_reply.started": "2025-03-16T21:17:25.637325Z"
        },
        "id": "REUDHWrWcuoN",
        "jupyter": {
          "outputs_hidden": false
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "\n",
        "#Layer Normalization\n",
        "\n",
        "class LayerNormalization(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        embeddings_dims = embeddings_dims\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.norm = nn.LayerNorm(normalized_shape=embeddings_dims)\n",
        "    def forward(self, x):\n",
        "\n",
        "        return self.norm(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 170,
      "metadata": {
        "_cell_guid": "762624a5-8d63-403b-bc4e-039b16aba3f9",
        "_uuid": "70301094-ba8a-4078-a1ad-46dc8d357f6a",
        "execution": {
          "iopub.execute_input": "2025-03-16T21:17:25.654302Z",
          "iopub.status.busy": "2025-03-16T21:17:25.654077Z",
          "iopub.status.idle": "2025-03-16T21:17:25.672472Z",
          "shell.execute_reply": "2025-03-16T21:17:25.671677Z",
          "shell.execute_reply.started": "2025-03-16T21:17:25.654281Z"
        },
        "id": "lEe02cH9cuoN",
        "jupyter": {
          "outputs_hidden": false
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "\n",
        "#FeedForward Neural Network\n",
        "\n",
        "class MLPBlock(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        dropout = dropout,\n",
        "        embeddings_size = embeddings_dims,\n",
        "        # inner_dimensional_states: int = 3072\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(device=device, in_features=embeddings_size, out_features= 4 * embeddings_dims),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(device=device, in_features= 4 * embeddings_dims, out_features=embeddings_size),\n",
        "            nn.Dropout(p = dropout)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # mlp_weights_init = self.mlp.apply(weights_init)\n",
        "        return self.mlp(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 171,
      "metadata": {
        "_cell_guid": "50c3f233-f3ea-44cc-bccf-10aa13fc35d5",
        "_uuid": "bd1f493a-c5eb-4821-938b-1fd4baf55549",
        "execution": {
          "iopub.execute_input": "2025-03-16T21:17:25.673679Z",
          "iopub.status.busy": "2025-03-16T21:17:25.673386Z",
          "iopub.status.idle": "2025-03-16T21:17:25.693459Z",
          "shell.execute_reply": "2025-03-16T21:17:25.692644Z",
          "shell.execute_reply.started": "2025-03-16T21:17:25.673647Z"
        },
        "id": "cf0Jf_7UcuoN",
        "jupyter": {
          "outputs_hidden": false
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "class MaskedAttentionHead(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        attn_dropout = attn_dropout,\n",
        "        embeddings_dims = embeddings_dims,\n",
        "        no_of_heads = no_of_heads,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.head_size = embeddings_dims // no_of_heads\n",
        "        self.query = nn.Linear(in_features=embeddings_dims, out_features=self.head_size, device=device, bias=False)\n",
        "        self.keys = nn.Linear(in_features=embeddings_dims, out_features=self.head_size,device=device, bias=False)\n",
        "        self.values = nn.Linear(in_features=embeddings_dims, out_features=self.head_size, device=device,bias=False)\n",
        "        self.dropout = nn.Dropout(p = attn_dropout)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        # print(x.shape)\n",
        "        batch, block_size, embd_dims = x.shape\n",
        "        k = self.keys(x)\n",
        "        q = self.query(x)\n",
        "        v = self.values(x)\n",
        "        masked_table = torch.tril(torch.ones(block_size, block_size, device=device))\n",
        "        weights = q @ torch.transpose(k, dim0=-2, dim1=-1) * (k.shape[-1] ** -0.5)\n",
        "        masked_values = weights.masked_fill(masked_table[: block_size, : block_size] == 0, float('-inf'))\n",
        "        weights_normalized = nn.functional.softmax(masked_values, dim=-1) #Normalize along the embeddings dimension for all the tokens\n",
        "        weights_normalized = self.dropout(weights_normalized)\n",
        "        out = weights_normalized @ v\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 172,
      "metadata": {
        "_cell_guid": "aee629bd-84ad-4f0c-a0e7-9e3070e10081",
        "_uuid": "a252848c-a0b1-4c08-8a42-e2851e642a06",
        "execution": {
          "iopub.execute_input": "2025-03-16T21:17:25.694740Z",
          "iopub.status.busy": "2025-03-16T21:17:25.694382Z",
          "iopub.status.idle": "2025-03-16T21:17:25.713823Z",
          "shell.execute_reply": "2025-03-16T21:17:25.712953Z",
          "shell.execute_reply.started": "2025-03-16T21:17:25.694708Z"
        },
        "id": "OUFERSL2u8LT",
        "jupyter": {
          "outputs_hidden": false
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "class MaskedMHA(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        attn_dropout = attn_dropout,\n",
        "        embeddings_dims = embeddings_dims,\n",
        "        no_of_heads = no_of_heads,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([MaskedAttentionHead(attn_dropout=attn_dropout, embeddings_dims=embeddings_dims, no_of_heads=no_of_heads) for _ in range(no_of_heads)])\n",
        "        self.dropout = nn.Dropout(p = attn_dropout)\n",
        "        self.linear = nn.Linear(in_features=embeddings_dims, out_features=embeddings_dims, device=device, bias=False) # 12 (no of heads) * (batch_size) 64 = 768 -> gives out the text embeddings\n",
        "\n",
        "    def forward(self, x):\n",
        "        concat = torch.cat([head(x) for head in self.heads], dim=-1)\n",
        "        linear_layer = self.linear(concat)\n",
        "        out = self.dropout(linear_layer)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 173,
      "metadata": {
        "_cell_guid": "76ee1dab-07cf-4ae4-ac9a-3c134b9dc41d",
        "_uuid": "0bdd11ba-b65a-4dbe-9b09-25d24c2b3ab6",
        "execution": {
          "iopub.execute_input": "2025-03-16T21:17:25.714859Z",
          "iopub.status.busy": "2025-03-16T21:17:25.714607Z",
          "iopub.status.idle": "2025-03-16T21:17:25.736087Z",
          "shell.execute_reply": "2025-03-16T21:17:25.735469Z",
          "shell.execute_reply.started": "2025-03-16T21:17:25.714838Z"
        },
        "id": "oGGyyF4pjHyd",
        "jupyter": {
          "outputs_hidden": false
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "\n",
        "#Single Attention Head\n",
        "\n",
        "class CrossAttentionHead(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        attn_dropout = attn_dropout,\n",
        "        embeddings_dims = embeddings_dims,\n",
        "        no_of_heads = no_of_heads,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.head_size = embeddings_dims // no_of_heads\n",
        "        self.query = nn.Linear(in_features=embeddings_dims, out_features=self.head_size, device=device, bias=False)\n",
        "        self.keys = nn.Linear(in_features=embeddings_dims, out_features=self.head_size,device=device, bias=False)\n",
        "        self.values = nn.Linear(in_features=embeddings_dims, out_features=self.head_size, device=device,bias=False)\n",
        "        self.dropout = nn.Dropout(p = attn_dropout)\n",
        "\n",
        "\n",
        "    def forward(self, query, key, value, mask=None):\n",
        "\n",
        "\n",
        "        batch, block_size, embd_dims = query.shape\n",
        "        q = self.query(query)\n",
        "        k = self.keys(key)\n",
        "        v = self.values(value)\n",
        "        # masked_table = torch.tril(torch.ones(block_size, block_size, device=device))\n",
        "        # weights = query @ torch.transpose(key, dim0=-2, dim1=-1) * (key.shape[-1] ** -0.5)\n",
        "        # if(mask != None):\n",
        "        #     mask = mask.unsqueeze(1)\n",
        "        #     masked_values = weights.masked_fill(mask == 0, float('-inf'))\n",
        "        #     weights_normalized = nn.functional.softmax(masked_values, dim=-1) #Normalize along the embeddings dimension for all the tokens\n",
        "        #     # weights_normalized = self.dropout(weights_normalized)\n",
        "        #     out = weights_normalized @ value\n",
        "        #     out = self.dropout(out)\n",
        "        #     return out\n",
        "        # else:\n",
        "        #     weights_normalized = nn.functional.softmax(weights, dim=-1) #Normalize along the embeddings dimension for all the tokens\n",
        "        #     # weights_normalized = self.dropout(weights_normalized)\n",
        "        #     out = weights_normalized @ value\n",
        "        #     out = self.dropout(out)\n",
        "        #     return out\n",
        "\n",
        "        masked_table = torch.tril(torch.ones(block_size, block_size, device=device))\n",
        "        weights = q @ torch.transpose(k, dim0=-2, dim1=-1) * (k.shape[-1] ** -0.5)\n",
        "        masked_values = weights.masked_fill(masked_table[: block_size, : block_size] == 0, float('-inf'))\n",
        "        weights_normalized = nn.functional.softmax(masked_values, dim=-1) #Normalize along the embeddings dimension for all the tokens\n",
        "        weights_normalized = self.dropout(weights_normalized)\n",
        "        out = weights_normalized @ v\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 174,
      "metadata": {
        "_cell_guid": "bcde57b5-61a0-4d56-aa5a-5fbbc2f2a9bd",
        "_uuid": "af55eb4d-6d72-465f-8692-4777da91e56f",
        "execution": {
          "iopub.execute_input": "2025-03-16T21:17:25.737123Z",
          "iopub.status.busy": "2025-03-16T21:17:25.736929Z",
          "iopub.status.idle": "2025-03-16T21:17:25.763686Z",
          "shell.execute_reply": "2025-03-16T21:17:25.762859Z",
          "shell.execute_reply.started": "2025-03-16T21:17:25.737106Z"
        },
        "id": "U5NmszzcjHyf",
        "jupyter": {
          "outputs_hidden": false
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "#Single Attention Head\n",
        "\n",
        "class FullAttentionHead(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        attn_dropout = attn_dropout,\n",
        "        embeddings_dims = embeddings_dims,\n",
        "        no_of_heads = no_of_heads,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.head_size = embeddings_dims // no_of_heads\n",
        "        self.query = nn.Linear(in_features=embeddings_dims, out_features=self.head_size, device=device, bias=False)\n",
        "        self.keys = nn.Linear(in_features=embeddings_dims, out_features=self.head_size,device=device, bias=False)\n",
        "        self.values = nn.Linear(in_features=embeddings_dims, out_features=self.head_size, device=device,bias=False)\n",
        "        self.dropout = nn.Dropout(p = attn_dropout)\n",
        "\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        # batch, block_size, embd_dims = x.shape\n",
        "        k = self.keys(x)\n",
        "        q = self.query(x)\n",
        "        v = self.values(x)\n",
        "        # masked_table = torch.tril(torch.ones(block_size, block_size, device=device))\n",
        "        weights = q @ torch.transpose(k, dim0=-2, dim1=-1) * (k.shape[-1] ** -0.5)\n",
        "        if(mask != None):\n",
        "            mask = mask.unsqueeze(1)\n",
        "            masked_values = weights.masked_fill(mask == 0, float('-inf'))\n",
        "            weights_normalized = nn.functional.softmax(masked_values, dim=-1) #Normalize along the embeddings dimension for all the tokens\n",
        "            # weights_normalized = self.dropout(weights_normalized)\n",
        "            out = weights_normalized @ v\n",
        "            out = self.dropout(out)\n",
        "            return out\n",
        "        else:\n",
        "            weights_normalized = nn.functional.softmax(weights, dim=-1) #Normalize along the embeddings dimension for all the tokens\n",
        "            # weights_normalized = self.dropout(weights_normalized)\n",
        "            out = weights_normalized @ v\n",
        "            out = self.dropout(out)\n",
        "            return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 175,
      "metadata": {
        "_cell_guid": "8099497c-1f19-4d53-8965-ae8f153335be",
        "_uuid": "50fee96b-7dae-4da2-951a-b06330293a3e",
        "execution": {
          "iopub.execute_input": "2025-03-16T21:17:25.765002Z",
          "iopub.status.busy": "2025-03-16T21:17:25.764640Z",
          "iopub.status.idle": "2025-03-16T21:17:25.787092Z",
          "shell.execute_reply": "2025-03-16T21:17:25.786217Z",
          "shell.execute_reply.started": "2025-03-16T21:17:25.764968Z"
        },
        "id": "v_BB7r7kqmOc",
        "jupyter": {
          "outputs_hidden": false
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "\n",
        "class FullMHA(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        attn_dropout = attn_dropout,\n",
        "        embeddings_dims = embeddings_dims,\n",
        "        no_of_heads = no_of_heads,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([FullAttentionHead(attn_dropout=attn_dropout, embeddings_dims=embeddings_dims, no_of_heads=no_of_heads) for _ in range(no_of_heads)])\n",
        "        self.dropout = nn.Dropout(p = attn_dropout)\n",
        "        self.linear = nn.Linear(in_features=embeddings_dims, out_features=embeddings_dims, device=device, bias=False) # 12 (no of heads) * (batch_size) 64 = 768 -> gives out the text embeddings\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        concat = torch.cat([head(x, mask) for head in self.heads], dim=-1)\n",
        "        linear_layer = self.linear(concat)\n",
        "        out = self.dropout(linear_layer)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 176,
      "metadata": {
        "_cell_guid": "06ee6670-5428-4e5a-867e-cdd98cddc2aa",
        "_uuid": "3627fa0f-bd35-4dab-9451-03a2b716732b",
        "execution": {
          "iopub.execute_input": "2025-03-16T21:17:25.788020Z",
          "iopub.status.busy": "2025-03-16T21:17:25.787815Z",
          "iopub.status.idle": "2025-03-16T21:17:25.809259Z",
          "shell.execute_reply": "2025-03-16T21:17:25.808685Z",
          "shell.execute_reply.started": "2025-03-16T21:17:25.788001Z"
        },
        "id": "TTwRkBzcvE-_",
        "jupyter": {
          "outputs_hidden": false
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "class CrossMHA(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        attn_dropout = attn_dropout,\n",
        "        embeddings_dims = embeddings_dims,\n",
        "        no_of_heads = no_of_heads,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([CrossAttentionHead(attn_dropout=attn_dropout, embeddings_dims=embeddings_dims, no_of_heads=no_of_heads) for _ in range(no_of_heads)])\n",
        "        self.dropout = nn.Dropout(p = attn_dropout)\n",
        "        self.linear = nn.Linear(in_features=embeddings_dims, out_features=embeddings_dims, device=device, bias=False)\n",
        "\n",
        "    def forward(self, value, key, x, mask=None):\n",
        "        concat = torch.cat([head(x, key, value,  mask) for head in self.heads], dim=-1)\n",
        "        linear_layer = self.linear(concat)\n",
        "        out = self.dropout(linear_layer)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 177,
      "metadata": {
        "_cell_guid": "2e095386-2ee5-4a3c-83bb-cad96cb26746",
        "_uuid": "2c290085-c1fe-4c64-9bab-baa448c3bae5",
        "execution": {
          "iopub.execute_input": "2025-03-16T21:17:25.810295Z",
          "iopub.status.busy": "2025-03-16T21:17:25.810050Z",
          "iopub.status.idle": "2025-03-16T21:17:25.826116Z",
          "shell.execute_reply": "2025-03-16T21:17:25.825190Z",
          "shell.execute_reply.started": "2025-03-16T21:17:25.810261Z"
        },
        "id": "s9rJzO_XcuoO",
        "jupyter": {
          "outputs_hidden": false
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Decoder Block\n",
        "\n",
        "class TransformerDecoderBlock(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        attn_dropout = attn_dropout,\n",
        "        embeddings_dims = embeddings_dims,\n",
        "        no_of_heads = no_of_heads,\n",
        "        dropout = dropout,\n",
        "        # vocab_size = vocab_size\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.cross = CrossMHA(attn_dropout=attn_dropout, embeddings_dims=embeddings_dims, no_of_heads=no_of_heads)\n",
        "        self.masked = MaskedMHA(attn_dropout=attn_dropout, embeddings_dims=embeddings_dims, no_of_heads=no_of_heads)\n",
        "        self.layer_norm1 = LayerNormalization(embeddings_dims)\n",
        "        self.layer_norm2 = LayerNormalization(embeddings_dims)\n",
        "        # self.layer_norm3 = LayerNormalization(embeddings_dims=embeddings_dims)\n",
        "        self.layer_norm4 = LayerNormalization(embeddings_dims)\n",
        "        self.mlp_block = MLPBlock(dropout=dropout, embeddings_size=embeddings_dims)\n",
        "\n",
        "    def forward(self, key, value, x, mask=None):\n",
        "        x = self.layer_norm1(x + self.masked(x)) #Very important step -> Layer Norm on input and then passes it to the subsequent blocks\n",
        "        # print(x.shape)\n",
        "        x = self.layer_norm2(x + self.cross(value, key, x, mask)) #Very important step\n",
        "        # print(x.shape)\n",
        "        # x = x + self.mha(self.layer_norm1(x))  #Very important step -> Layer Norm on input and then passes it to the subsequent blocks\n",
        "        x = self.layer_norm4(x + self.mlp_block(x)) #Very important step\n",
        "        # print(x.shape)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class PostNet(nn.Module):\n",
        "  def __init__(self):\n",
        "\n",
        "    super().__init__()\n",
        "\n",
        "    self.out = embeddings_dims\n",
        "    self.device = device\n",
        "    self.conv_layer1 = nn.Conv1d(self.out, self.out, kernel_size=kernel_size, device=self.device, padding=1)\n",
        "    self.conv_layer2 = nn.Conv1d(self.out, self.out, kernel_size=kernel_size, device=self.device, padding=1)\n",
        "    self.conv_layer3 = nn.Conv1d(self.out, self.out, kernel_size=kernel_size, device=self.device, padding=1)\n",
        "    self.conv_layer4 = nn.Conv1d(self.out, self.out, kernel_size=kernel_size, device=self.device, padding=1)\n",
        "    self.conv_layer5 = nn.Conv1d(self.out, self.out, kernel_size=kernel_size, device=self.device, padding=1)\n",
        "    self.norm = torch.nn.BatchNorm1d(self.out)\n",
        "  def forward(self,x):\n",
        "    x = self.conv_layer1(x)\n",
        "    x = self.norm(x)\n",
        "    x = torch.nn.functional.tanh(x)\n",
        "    x = self.conv_layer2(x)\n",
        "    x = self.norm(x)\n",
        "    x = torch.nn.functional.tanh(x)\n",
        "    x = self.conv_layer3(x)\n",
        "    x = self.norm(x)\n",
        "    x = torch.nn.functional.tanh(x)\n",
        "    x = self.conv_layer4(x)\n",
        "    x = self.norm(x)\n",
        "    x = torch.nn.functional.tanh(x)\n",
        "    x = self.conv_layer5(x)\n",
        "    x = self.norm(x)\n",
        "    return x\n"
      ],
      "metadata": {
        "id": "PjbaRskFRHkN"
      },
      "execution_count": 178,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 179,
      "metadata": {
        "_cell_guid": "63e5400a-b5d2-4ef7-8f30-b1bc0e233820",
        "_uuid": "05fd4f49-cc4b-474c-9a9c-3d5191eefa43",
        "execution": {
          "iopub.execute_input": "2025-03-16T21:17:25.827199Z",
          "iopub.status.busy": "2025-03-16T21:17:25.826969Z",
          "iopub.status.idle": "2025-03-16T21:17:25.850583Z",
          "shell.execute_reply": "2025-03-16T21:17:25.849957Z",
          "shell.execute_reply.started": "2025-03-16T21:17:25.827178Z"
        },
        "id": "KGh8ujQJcuoO",
        "jupyter": {
          "outputs_hidden": false
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Decoder Block\n",
        "\n",
        "class DecoderModel(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        attn_dropout = attn_dropout,\n",
        "        embeddings_dims = embeddings_dims,\n",
        "        no_of_heads = no_of_heads,\n",
        "        block_size = block_size,\n",
        "        dropout = dropout,\n",
        "        no_of_decoder_layers = no_of_decoder_layers,\n",
        "        # vocab_size = vocab_size\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        # self.tgt_text_embds = TgtTextEmbeddings(vocab_size=tgt_vocab_size, embeddings_dims=embeddings_dims)\n",
        "        # self.linear_layer = nn.Linear(in_features=embeddings_dims, out_features=tgt_vocab_size, device=device, bias=False) # Takes in logits of dimensions- embeds_dims and converts it into dimension of vocab_size (logits in range of vocab_size)\n",
        "        # self.layer_norm = LayerNormalization(embeddings_dims=embeddings_dims)\n",
        "        self.decoder_layers = nn.ModuleList([TransformerDecoderBlock(attn_dropout=attn_dropout, embeddings_dims=embeddings_dims, no_of_heads=no_of_heads, dropout=dropout) for _ in range(no_of_decoder_layers)])\n",
        "        self.apply(self._init_weights)\n",
        "        # self.positional_embeddings_tgt = nn.Parameter(torch.randn(1, block_size, embeddings_dims, device=device), requires_grad=True) #To give positional embeddings to each token of the input text, hence num_embeddings=block_size\n",
        "        self.positional_embeddings_tgt = PositionEmbeddings()\n",
        "        self.scaled_factor = nn.Parameter(torch.ones(1, block_size, embeddings_dims), requires_grad=True)\n",
        "        # torch.nn.init.normal_(self.positional_embeddings_tgt, mean=0.0, std=0.02)\n",
        "\n",
        "        # out = self.decoder_layers(query, key, x)\n",
        "        # Loop through each decoder layer\n",
        "    def _init_weights(self, module):  #Weight Initialization\n",
        "            if isinstance(module, nn.Linear):\n",
        "                torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "                if module.bias is not None:\n",
        "                    torch.nn.init.zeros_(module.bias)\n",
        "            elif isinstance(module, nn.Embedding):\n",
        "                torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "    def forward(self, key, value, x, mask):\n",
        "        # x = self.tgt_text_embds(x)\n",
        "        x = x + self.scaled_factor * self.positional_embeddings_tgt(x)\n",
        "        # print(x.shape)\n",
        "        for decoder_layer in self.decoder_layers:\n",
        "            x = decoder_layer(key, value, x, mask)\n",
        "        # x = self.layer_norm(x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 180,
      "metadata": {
        "_cell_guid": "5327372f-0db3-4ef1-a59c-1757b6edfc62",
        "_uuid": "1e94a41d-aaa3-4e74-a02f-fb0edbfa9118",
        "execution": {
          "iopub.execute_input": "2025-03-16T21:17:25.854582Z",
          "iopub.status.busy": "2025-03-16T21:17:25.854371Z",
          "iopub.status.idle": "2025-03-16T21:17:25.876149Z",
          "shell.execute_reply": "2025-03-16T21:17:25.875339Z",
          "shell.execute_reply.started": "2025-03-16T21:17:25.854563Z"
        },
        "id": "A3SgKrC-jHyd",
        "jupyter": {
          "outputs_hidden": false
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "\n",
        "#Encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 181,
      "metadata": {
        "_cell_guid": "2a3683d1-303c-41a2-9cc6-b7e053fde34c",
        "_uuid": "3ecc13cd-e38a-4e1c-af9c-10f8e84b02d7",
        "execution": {
          "iopub.execute_input": "2025-03-16T21:17:25.878065Z",
          "iopub.status.busy": "2025-03-16T21:17:25.877823Z",
          "iopub.status.idle": "2025-03-16T21:17:25.893369Z",
          "shell.execute_reply": "2025-03-16T21:17:25.892523Z",
          "shell.execute_reply.started": "2025-03-16T21:17:25.878033Z"
        },
        "id": "v6mbbO3yp-gh",
        "jupyter": {
          "outputs_hidden": false
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "class TransformerEncoderBlock(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        attn_dropout = attn_dropout,\n",
        "        embeddings_dims = embeddings_dims,\n",
        "        no_of_heads = no_of_heads,\n",
        "        dropout = dropout,\n",
        "        mask=None\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.mha = FullMHA(attn_dropout=attn_dropout, embeddings_dims=embeddings_dims, no_of_heads=no_of_heads)\n",
        "        self.layer_norm1 = LayerNormalization(embeddings_dims)\n",
        "        self.layer_norm2 = LayerNormalization(embeddings_dims)\n",
        "        self.mlp_block = MLPBlock(dropout=dropout, embeddings_size=embeddings_dims)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        x = self.layer_norm1(x + self.mha(x, mask))\n",
        "        x = self.layer_norm2(x + self.mlp_block(x))\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 193,
      "metadata": {
        "_cell_guid": "2fa82426-e551-497d-8247-051a55f74a3c",
        "_uuid": "39475274-a828-4151-9556-fa4bf30d4455",
        "execution": {
          "iopub.execute_input": "2025-03-16T21:17:25.894660Z",
          "iopub.status.busy": "2025-03-16T21:17:25.894341Z",
          "iopub.status.idle": "2025-03-16T21:17:25.916685Z",
          "shell.execute_reply": "2025-03-16T21:17:25.915829Z",
          "shell.execute_reply.started": "2025-03-16T21:17:25.894605Z"
        },
        "id": "HxW0pvnV12Ms",
        "jupyter": {
          "outputs_hidden": false
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "class EncoderModel(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        attn_dropout = attn_dropout,\n",
        "        embeddings_dims = embeddings_dims,\n",
        "        no_of_heads = no_of_heads,\n",
        "        block_size = block_size,\n",
        "        dropout = dropout,\n",
        "        no_of_decoder_layers = no_of_decoder_layers,\n",
        "        # vocab_size = vocab_size\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        # self.positional_embeddings_src = nn.Parameter(torch.randn(1, block_size, embeddings_dims, device=device), requires_grad=True) #To give positional embeddings to each token of the input text, hence num_embeddings=block_size\n",
        "        self.prenet_enc = PrenetEncoder()\n",
        "        self.pos_embeds = nn.Parameter(torch.randn(1, block_size, embeddings_dims, device=device), requires_grad=True)\n",
        "        self.trainable_factor = nn.Parameter(torch.ones(1, block_size, embeddings_dims, device=device), requires_grad=True)\n",
        "        # self.conv1 = nn.Conv1d(in_channels=n_channels, out_channels=embeddings_dims, kernel_size=kernel_size, device=device, padding=1)\n",
        "        # self.conv2 = nn.Conv1d(in_channels=embeddings_dims, out_channels=embeddings_dims, kernel_size=kernel_size, device=device, padding=1)\n",
        "\n",
        "        self.positional_embeddings_src = PositionEmbeddings()\n",
        "        self.src_text_embeds = nn.Embedding(num_embeddings=src_vocab_size, embedding_dim=embeddings_dims, device=device)\n",
        "        # self.src_text_embeds = SrcTextEmbeddings(vocab_size=src_vocab_size, embeddings_dims=embeddings_dims)\n",
        "        self.encoder_layers = nn.ModuleList([TransformerEncoderBlock(attn_dropout=attn_dropout, embeddings_dims=embeddings_dims, no_of_heads=no_of_heads, dropout=dropout) for _ in range(no_of_decoder_layers)])\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):  #Weight Initialization\n",
        "            if isinstance(module, nn.Linear):\n",
        "                torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "                if module.bias is not None:\n",
        "                    torch.nn.init.zeros_(module.bias)\n",
        "            elif isinstance(module, nn.Embedding):\n",
        "                torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "\n",
        "        # x = self.conv1(x)\n",
        "        # x = torch.nn.functional.gelu(x)\n",
        "        # x = self.conv2(x)\n",
        "        # x = torch.nn.functional.gelu(x)\n",
        "        # print(x.shape)\n",
        "        x = self.src_text_embeds(x)\n",
        "        # print(self.positional_embeddings_src.shape)\n",
        "        x = x.transpose(1, 2).contiguous()\n",
        "        x = self.prenet_enc(x)\n",
        "        x = x.permute(0, 2, 1)\n",
        "        # print(x.shape)\n",
        "        # print(self.positional_embeddings_src(x).shape)\n",
        "        x = x + self.trainble_factor * self.positional_embeddings_src(x)\n",
        "        # print(x)\n",
        "        # print(x.shape)\n",
        "        # Loop through each encoder layer\n",
        "        for encoder_layer in self.encoder_layers:\n",
        "            x = encoder_layer(x, mask)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 193,
      "metadata": {
        "_cell_guid": "ad3a1595-5722-4fff-a54e-87e3bb755abe",
        "_uuid": "249cf674-9b8b-406a-920d-603c6c2f00ff",
        "id": "Qi3v6jczY3go",
        "jupyter": {
          "outputs_hidden": false
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 194,
      "metadata": {
        "_cell_guid": "a6d4d773-0ed2-4059-ab2b-683692ce2c32",
        "_uuid": "6f0e32f9-14f2-4b65-a106-cab2051ad125",
        "execution": {
          "iopub.execute_input": "2025-03-16T21:17:25.917942Z",
          "iopub.status.busy": "2025-03-16T21:17:25.917671Z",
          "iopub.status.idle": "2025-03-16T21:17:25.937565Z",
          "shell.execute_reply": "2025-03-16T21:17:25.936953Z",
          "shell.execute_reply.started": "2025-03-16T21:17:25.917913Z"
        },
        "id": "2UWijIFl2Ykd",
        "jupyter": {
          "outputs_hidden": false
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "class TTS(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.encoder = EncoderModel()\n",
        "        self.decoder = DecoderModel()\n",
        "        self.postnet = PostNet()\n",
        "        # self.pos = PositionalEmbeddings()\n",
        "        # self.tgt_text_embds = TgtTextEmbeddings(vocab_size=tgt_vocab_size, embeddings_dims=embeddings_dims)\n",
        "        self.linear_layer = nn.Linear(in_features=embeddings_dims, out_features=n_channels, device=device, bias=False) # Takes in logits of dimensions- embeds_dims and converts it into dimension of vocab_size (logits in range of vocab_size)\n",
        "        # self.src_text_embeds = SrcTextEmbeddings(vocab_size=src_vocab_size, embeddings_dims=embeddings_dims)\n",
        "        self.stop_layer = nn.Linear(in_features=embeddings_dims, out_features=1, device=device, bias=False)\n",
        "    def forward(self, src, tgt, src_mask=None, tgt_mask=None):\n",
        "        # x = self.src_text_embeds(src)\n",
        "        x = self.encoder(src, src_mask)\n",
        "        # y = self.tgt_text_embds(tgt)\n",
        "        # print(x.shape)\n",
        "        y = self.decoder(x, x, y, tgt_mask)\n",
        "        # print(y.shape)\n",
        "\n",
        "        out1 = self.linear_layer(y)\n",
        "        out2 = self.postnet(out1)\n",
        "        out2 += out1\n",
        "        stop = self.stop_layer(y)\n",
        "        return out1, out2, stop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 195,
      "metadata": {
        "_cell_guid": "17dd187f-c195-44a0-990c-c6d17c90227e",
        "_uuid": "e407283d-b631-479c-9bae-014aab7a0a3d",
        "execution": {
          "iopub.execute_input": "2025-03-16T21:17:25.938678Z",
          "iopub.status.busy": "2025-03-16T21:17:25.938378Z",
          "iopub.status.idle": "2025-03-16T21:17:26.442178Z",
          "shell.execute_reply": "2025-03-16T21:17:26.441341Z",
          "shell.execute_reply.started": "2025-03-16T21:17:25.938647Z"
        },
        "id": "ntIaQj1U3pFX",
        "jupyter": {
          "outputs_hidden": false
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "#Instantiating the model\n",
        "model = TTS()\n",
        "# model = torch.compile(model)\n",
        "# model = model.to(device)\n",
        "model = model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "4f4d8f86-6a6c-43fe-8a7f-06301998ac15",
        "_uuid": "3efcd10c-fbd4-4818-b23b-636503f4df30",
        "execution": {
          "iopub.execute_input": "2025-03-16T21:17:26.443631Z",
          "iopub.status.busy": "2025-03-16T21:17:26.443241Z",
          "iopub.status.idle": "2025-03-16T21:17:26.730652Z",
          "shell.execute_reply": "2025-03-16T21:17:26.729694Z",
          "shell.execute_reply.started": "2025-03-16T21:17:26.443574Z"
        },
        "jupyter": {
          "outputs_hidden": false
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y_T--TYbpsOq",
        "outputId": "79609e28-c3e5-444c-b996-2503a3b43c28"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ],
      "source": [
        "import gc\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "130524f5-869f-4ea2-8cb7-5de2780ed66a",
        "_uuid": "3ecf1b70-4a77-4786-bdbd-df86011c7d31",
        "execution": {
          "iopub.execute_input": "2025-03-16T21:17:26.732102Z",
          "iopub.status.busy": "2025-03-16T21:17:26.731735Z",
          "iopub.status.idle": "2025-03-16T21:17:26.758778Z",
          "shell.execute_reply": "2025-03-16T21:17:26.757829Z",
          "shell.execute_reply.started": "2025-03-16T21:17:26.732063Z"
        },
        "jupyter": {
          "outputs_hidden": false
        },
        "trusted": true,
        "id": "7wQoqS77psOr"
      },
      "outputs": [],
      "source": [
        "# tgt_mask = torch.randint(1, tgt_vocab_size, (batch_size, block_size)).to(device)  #\n",
        "# tgt_mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 196,
      "metadata": {
        "_cell_guid": "d92d3629-d59c-4361-9572-8e0075329808",
        "_uuid": "5061d499-36a9-427b-90ad-eead117d5a51",
        "execution": {
          "iopub.execute_input": "2025-03-16T21:17:26.759979Z",
          "iopub.status.busy": "2025-03-16T21:17:26.759689Z",
          "iopub.status.idle": "2025-03-16T21:17:31.629859Z",
          "shell.execute_reply": "2025-03-16T21:17:31.628564Z",
          "shell.execute_reply.started": "2025-03-16T21:17:26.759955Z"
        },
        "id": "yOXtmG-lcuoO",
        "jupyter": {
          "outputs_hidden": false
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "0d92d194-c425-486d-b8ff-36e956e6872a"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Failed to run torchinfo. See above stack traces for more details. Executed layers up to: [Embedding: 2]",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchinfo/torchinfo.py\u001b[0m in \u001b[0;36mforward_pass\u001b[0;34m(model, x, batch_dim, cache_forward_pass, device, mode, **kwargs)\u001b[0m\n\u001b[1;32m    294\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m                 \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1844\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1845\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1846\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36minner\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1792\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1793\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1794\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0m_global_forward_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-194-5d015d06a689>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src, tgt, src_mask, tgt_mask)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;31m# x = self.src_text_embeds(src)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0;31m# y = self.tgt_text_embds(tgt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1844\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1845\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1846\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36minner\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1792\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1793\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1794\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0m_global_forward_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-193-43daaed8b784>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, mask)\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprenet_enc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1844\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1845\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1846\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36minner\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1792\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1793\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1794\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0m_global_forward_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-166-1e935ed73b44>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1844\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1845\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1846\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36minner\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1792\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1793\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1794\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0m_global_forward_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    374\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 375\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    376\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    369\u001b[0m             )\n\u001b[0;32m--> 370\u001b[0;31m         return F.conv1d(\n\u001b[0m\u001b[1;32m    371\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdilation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Expected 4-dimensional input for 4-dimensional weight [512, 512, 5, 1], but got 3-dimensional input of size [64, 512, 64] instead",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-196-87c4cc66148b>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m summary(model=model,\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0minput_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspec1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mcol_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"input_size\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"output_size\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"num_params\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"trainable\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchinfo/torchinfo.py\u001b[0m in \u001b[0;36msummary\u001b[0;34m(model, input_size, input_data, batch_dim, cache_forward_pass, col_names, col_width, depth, device, dtypes, mode, row_settings, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    221\u001b[0m         \u001b[0minput_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m     )\n\u001b[0;32m--> 223\u001b[0;31m     summary_list = forward_pass(\n\u001b[0m\u001b[1;32m    224\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache_forward_pass\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_mode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchinfo/torchinfo.py\u001b[0m in \u001b[0;36mforward_pass\u001b[0;34m(model, x, batch_dim, cache_forward_pass, device, mode, **kwargs)\u001b[0m\n\u001b[1;32m    302\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m         \u001b[0mexecuted_layers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlayer\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msummary_list\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuted\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 304\u001b[0;31m         raise RuntimeError(\n\u001b[0m\u001b[1;32m    305\u001b[0m             \u001b[0;34m\"Failed to run torchinfo. See above stack traces for more details. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m             \u001b[0;34mf\"Executed layers up to: {executed_layers}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Failed to run torchinfo. See above stack traces for more details. Executed layers up to: [Embedding: 2]"
          ]
        }
      ],
      "source": [
        "\n",
        "# !pip install torchinfo\n",
        "from torchinfo import summary\n",
        "\n",
        "data = next(iter(train_dataloader))\n",
        "# print(data)\n",
        "# tgt_mask = torch.randint(1, tgt_vocab_size, (batch_size, block_size)).to(device)  #\n",
        "spec1 = data['spectrograms'].to(device)\n",
        "text = data['input_ids'].to(device)\n",
        "\n",
        "summary(model=model,\n",
        "        input_data=(text, spec1),\n",
        "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
        "        col_width=20,\n",
        "        row_settings=[\"var_names\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "97226398-cb33-48d6-9abc-f5a9c549b5c5",
        "_uuid": "ee98acba-6992-4b1e-8cc1-3eda3997ae5b",
        "execution": {
          "iopub.execute_input": "2025-03-16T21:18:47.480394Z",
          "iopub.status.busy": "2025-03-16T21:18:47.480025Z",
          "iopub.status.idle": "2025-03-16T21:18:47.490347Z",
          "shell.execute_reply": "2025-03-16T21:18:47.489487Z",
          "shell.execute_reply.started": "2025-03-16T21:18:47.480363Z"
        },
        "id": "LH95cJEvcuoO",
        "jupyter": {
          "outputs_hidden": false
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "\n",
        "# # Optimizer setup and scheduler steup\n",
        "# out = {\"Train\": None, \"val\": None}\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=max_lr)\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "53e031da-134a-4271-8879-b012f262ccc6",
        "_uuid": "7860931f-c816-4796-b4c5-265e3df9bf57",
        "execution": {
          "iopub.execute_input": "2025-03-16T21:18:47.977592Z",
          "iopub.status.busy": "2025-03-16T21:18:47.977290Z",
          "iopub.status.idle": "2025-03-16T21:18:47.981528Z",
          "shell.execute_reply": "2025-03-16T21:18:47.980718Z",
          "shell.execute_reply.started": "2025-03-16T21:18:47.977569Z"
        },
        "id": "bbvONdUTWmvL",
        "jupyter": {
          "outputs_hidden": false
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "torch.set_float32_matmul_precision('high')\n",
        "\n",
        "scaler = torch.amp.GradScaler(enabled=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "58095701-5ad6-4bd6-bc30-0b1422af2a93",
        "_uuid": "6d9f8456-d5da-49d1-8211-c97897782909",
        "execution": {
          "iopub.execute_input": "2025-03-16T21:18:48.014114Z",
          "iopub.status.busy": "2025-03-16T21:18:48.013911Z",
          "iopub.status.idle": "2025-03-16T21:18:48.018021Z",
          "shell.execute_reply": "2025-03-16T21:18:48.017242Z",
          "shell.execute_reply.started": "2025-03-16T21:18:48.014096Z"
        },
        "jupyter": {
          "outputs_hidden": false
        },
        "trusted": true,
        "id": "MdhqasdjpsOr"
      },
      "outputs": [],
      "source": [
        "def _save_snapshot(model, optimizer, scheduler, epoch, step):\n",
        "    snapshot = {\n",
        "        \"MODEL_STATE\": model.state_dict(),\n",
        "        \"OPTIMIZER_STATE\": optimizer.state_dict(),\n",
        "        # \"SCHEDULER_STATE\": scheduler.state_dict(),\n",
        "        \"EPOCHS_RUN\": epoch,\n",
        "        \"STEP_RUN\": step\n",
        "    }\n",
        "    torch.save(snapshot, f\"snapshot_{step}.pt\")\n",
        "    print(f\"Epoch: {epoch} | Step: {step} | Snapshot saved.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "6f951a8f-3f05-4b6a-9080-99a81e5925b0",
        "_uuid": "9f8791fb-3c63-4871-8402-fbfb2a649e89",
        "execution": {
          "iopub.execute_input": "2025-03-16T21:18:48.045122Z",
          "iopub.status.busy": "2025-03-16T21:18:48.044918Z",
          "iopub.status.idle": "2025-03-16T21:18:48.048197Z",
          "shell.execute_reply": "2025-03-16T21:18:48.047524Z",
          "shell.execute_reply.started": "2025-03-16T21:18:48.045104Z"
        },
        "jupyter": {
          "outputs_hidden": false
        },
        "trusted": true,
        "id": "wBo4MmRopsOr"
      },
      "outputs": [],
      "source": [
        "# !pip install torchtriton"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "2fc4d2f7-40d4-4733-8e32-65c38f198b5e",
        "_uuid": "5582f78b-b57d-484e-bfa5-66a2688a6e21",
        "execution": {
          "iopub.execute_input": "2025-03-16T21:18:48.069834Z",
          "iopub.status.busy": "2025-03-16T21:18:48.069579Z",
          "iopub.status.idle": "2025-03-16T21:18:48.074196Z",
          "shell.execute_reply": "2025-03-16T21:18:48.073418Z",
          "shell.execute_reply.started": "2025-03-16T21:18:48.069814Z"
        },
        "jupyter": {
          "outputs_hidden": false
        },
        "trusted": true,
        "id": "EpyoWzdxpsOr"
      },
      "outputs": [],
      "source": [
        "save_chechpoint_iter = 50\n",
        "total_iters = 20000\n",
        "eval_iters = 50\n",
        "eval_check = 100\n",
        "warmup_iters = 2048\n",
        "min_lr = 0.1 * max_lr\n",
        "lr_decay_iters = 20000\n",
        "total_batch_size = 524288\n",
        "micro_batch_size = batch_size\n",
        "gradient_accumulation_steps = total_batch_size // (micro_batch_size * (block_size * torch.cuda.device_count()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "20e917d1-9770-456a-99dd-5e9c33d4edda",
        "_uuid": "a0929ebe-c656-48ba-b479-e08113f3113e",
        "execution": {
          "iopub.execute_input": "2025-03-16T21:18:48.108479Z",
          "iopub.status.busy": "2025-03-16T21:18:48.108253Z",
          "iopub.status.idle": "2025-03-16T21:18:48.126877Z",
          "shell.execute_reply": "2025-03-16T21:18:48.126223Z",
          "shell.execute_reply.started": "2025-03-16T21:18:48.108459Z"
        },
        "jupyter": {
          "outputs_hidden": false
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oATkWQfApsOs",
        "outputId": "7e06c1e1-5d5b-4870-9bf1-5edabb88e42b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaders ready both\n"
          ]
        }
      ],
      "source": [
        "model.eval()\n",
        "world_size = torch.cuda.device_count()\n",
        "@torch.inference_mode()\n",
        "def estimate_loss(val_loader, val_iterator, device):\n",
        "    out = {}\n",
        "    # train_loader = prepare_dataset('train', ModelArgs.batch_size)\n",
        "\n",
        "    # val_loader_iterator = iter(val_loader)\n",
        "    loader = None\n",
        "    epoch_loss = None\n",
        "    epoch_losses = []\n",
        "    # print(\"Starting the eval...\")\n",
        "    for split in ['val']:\n",
        "        print(f\"Starting with {split} evaluation...\")\n",
        "        # losses = torch.zeros(ModelArgs.val_epochs)\n",
        "        # if(split == 'train'):\n",
        "        #         loader = train_loader\n",
        "        # if(split == 'val'):\n",
        "        #         loader = val_loader\n",
        "        for step in range(eval_check):\n",
        "            try:\n",
        "                batch = next(val_iterator)\n",
        "            except StopIteration:\n",
        "                val_loader_iterator = iter(val_loader)\n",
        "                X, x, y = next(val_loader_iterator)\n",
        "\n",
        "            # tgt_mask = torch.randint(1, tgt_vocab_size, (batch_size, block_size)).to(device)  #\n",
        "            total_loss = 0\n",
        "            # loader.sampler.set_epoch(step)\n",
        "            total_batches = 0\n",
        "            # batch = next(val_loader_iterator)\n",
        "            # for batch in loader:  # Loop through DataLoader batches\n",
        "            # idx = batch['input_ids']\n",
        "            # targets = batch['labels']\n",
        "            spec = X.to(device)\n",
        "\n",
        "            idx = x.to(device)\n",
        "            targets = y.to(device)\n",
        "            with torch.autocast(device_type=device, dtype=torch.float16):\n",
        "\n",
        "                logits = model(spec, idx)\n",
        "                batch_size, block_size, embeddings_dims = logits.shape\n",
        "                logits = logits.view(batch_size * block_size, embeddings_dims)  # Flatten tokens\n",
        "                targets = targets.view(batch_size * block_size)\n",
        "\n",
        "                loss = torch.nn.functional.cross_entropy(logits, targets, ignore_index=tokenizer.pad_token_id)\n",
        "\n",
        "                total_loss += loss.item()\n",
        "                total_batches += 1\n",
        "\n",
        "        # Compute mean loss for this epoch\n",
        "        epoch_loss = total_loss / total_batches if total_batches > 0 else 0.0\n",
        "        epoch_losses.append(epoch_loss)\n",
        "\n",
        "            # print(f\"Epoch {epoch + 1}/{ModelArgs.val_epochs}: Loss = {epoch_loss:.4f}\")\n",
        "\n",
        "        # Compute mean loss across all evaluation epochs\n",
        "        out[split] = sum(epoch_losses) / len(epoch_losses) if epoch_losses else 0.0\n",
        "        epoch_loss = None\n",
        "        epoch_losses = []\n",
        "\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "# model = model.to(rank)\n",
        "model.train()\n",
        "count = 0\n",
        "\n",
        "# train_dataloader = prepare_dataset('train', device, ModelArgs.batch_size)\n",
        "# val_loader= prepare_dataset('val', device, ModelArgs.batch_size)\n",
        "# for step in tqdm(range(total_iters)):\n",
        "# for epoch in range(ModelArgs.epochs):\n",
        "    # torch.cuda.synchronize()\n",
        "\n",
        "# train_dataloader.sampler.set_epoch(epoch)\n",
        "\n",
        "# val_loader.sampler.set_epoch(epoch)\n",
        "print(\"Loaders ready both\")\n",
        "epochs = epochs\n",
        "\n",
        "# train_step_iterator = range(len(train_dataloader))\n",
        "# if device == 0:  # Only create progress bar on rank 0\n",
        "#   train_step_iterator = tqdm(train_step_iterator, desc=\"Training Progress\", position=0, leave=True)\n",
        "\n",
        "    # Print progress on rank 0\n",
        "train_loader_length = 0\n",
        "train_data_iterator = iter(train_dataloader)\n",
        "val_data_iterator = iter(val_dataloader)\n",
        "token_count = 0\n",
        "if(device == 0):\n",
        "    train_loader_length = len(train_dataloader)\n",
        "    # print(\"Total batches: \", train_loader_length)\n",
        "# print(\"Length of : \", len(train_dataloader))\n",
        "# print(\"Length of val: \", len(val_loader))\n",
        "# for  step, batch in enumerate(train_dataloader):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "5fd44e9d-01f8-4e2b-9130-4640fdab7419",
        "_uuid": "c8abd50d-6173-4aeb-8219-b6396120af54",
        "execution": {
          "iopub.execute_input": "2025-03-16T21:18:48.145768Z",
          "iopub.status.busy": "2025-03-16T21:18:48.145554Z",
          "iopub.status.idle": "2025-03-16T21:18:48.149258Z",
          "shell.execute_reply": "2025-03-16T21:18:48.148608Z",
          "shell.execute_reply.started": "2025-03-16T21:18:48.145750Z"
        },
        "jupyter": {
          "outputs_hidden": false
        },
        "trusted": true,
        "id": "R9rFUrIlpsOs"
      },
      "outputs": [],
      "source": [
        "def find_unused_parameters(model):\n",
        "    unused = []\n",
        "    for name, param in model.named_parameters():\n",
        "        if param.grad is None:\n",
        "            unused.append(name)\n",
        "    return unused"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "39ba72e2-ffe8-4acf-afa3-dbc3077e3e2a",
        "_uuid": "1cdf81be-4ae9-43f2-b3c5-304c73e13883",
        "execution": {
          "iopub.execute_input": "2025-03-16T21:18:48.182998Z",
          "iopub.status.busy": "2025-03-16T21:18:48.182799Z",
          "iopub.status.idle": "2025-03-16T21:18:48.187182Z",
          "shell.execute_reply": "2025-03-16T21:18:48.186415Z",
          "shell.execute_reply.started": "2025-03-16T21:18:48.182979Z"
        },
        "jupyter": {
          "outputs_hidden": false
        },
        "trusted": true,
        "id": "AMnlZeKTpsOt"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "def get_lr(it):\n",
        "    # 1) linear warmup for warmup_iters steps\n",
        "    if it < warmup_iters:\n",
        "        return max_lr * (it + 1) / (warmup_iters + 1)\n",
        "    # 2) if it > lr_decay_iters, return min learning rate\n",
        "    if it > lr_decay_iters:\n",
        "        return min_lr\n",
        "    # 3) in between, use cosine decay down to min learning rate\n",
        "    decay_ratio = (it - warmup_iters) / (lr_decay_iters - warmup_iters)\n",
        "    assert 0 <= decay_ratio <= 1\n",
        "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio))\n",
        "    return min_lr + coeff * (max_lr - min_lr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "317d13ff-dc6c-4965-adfe-d28c4bd6b40d",
        "_uuid": "5feb1c4e-332c-4b97-8952-4646e07467a6",
        "execution": {
          "iopub.execute_input": "2025-03-16T21:18:48.204188Z",
          "iopub.status.busy": "2025-03-16T21:18:48.203969Z",
          "iopub.status.idle": "2025-03-16T21:18:48.207464Z",
          "shell.execute_reply": "2025-03-16T21:18:48.206860Z",
          "shell.execute_reply.started": "2025-03-16T21:18:48.204168Z"
        },
        "jupyter": {
          "outputs_hidden": false
        },
        "trusted": true,
        "id": "O7-thMpYpsOt"
      },
      "outputs": [],
      "source": [
        "#"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "44ced50a-b954-4b04-8bc2-25b7c2601b9c",
        "_uuid": "600fec50-d0e3-4593-a316-5c25e586c4f2",
        "execution": {
          "iopub.execute_input": "2025-03-16T21:18:48.265086Z",
          "iopub.status.busy": "2025-03-16T21:18:48.264869Z"
        },
        "id": "nPrSPPu8cuoO",
        "jupyter": {
          "outputs_hidden": false
        },
        "outputId": "27b849a0-ac60-4323-8183-bee0281e125e",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mrajceo2031\u001b[0m (\u001b[33mrentio\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250317_153549-bg6nilpi</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/rentio/Whisper-From-Scratch/runs/bg6nilpi' target=\"_blank\">elated-durian-22</a></strong> to <a href='https://wandb.ai/rentio/Whisper-From-Scratch' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/rentio/Whisper-From-Scratch' target=\"_blank\">https://wandb.ai/rentio/Whisper-From-Scratch</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/rentio/Whisper-From-Scratch/runs/bg6nilpi' target=\"_blank\">https://wandb.ai/rentio/Whisper-From-Scratch/runs/bg6nilpi</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/20000 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step :  0 / 20000\n",
            "Total batches:  343\n",
            "Total gradient accumulation steps:  128\n",
            "Micro Batch :  0\n",
            "Step :  0 / 20000\n",
            "Total batches:  343\n",
            "Total gradient accumulation steps:  128\n",
            "Total tokens processed:  0\n",
            "Micro Batch :  10\n",
            "Step :  0 / 20000\n",
            "Total batches:  343\n",
            "Total gradient accumulation steps:  128\n",
            "Total tokens processed:  0\n",
            "Micro Batch :  20\n",
            "Step :  0 / 20000\n",
            "Total batches:  343\n",
            "Total gradient accumulation steps:  128\n",
            "Total tokens processed:  0\n",
            "Micro Batch :  30\n",
            "Step :  0 / 20000\n",
            "Total batches:  343\n",
            "Total gradient accumulation steps:  128\n",
            "Total tokens processed:  0\n",
            "Micro Batch :  40\n",
            "Step :  0 / 20000\n",
            "Total batches:  343\n",
            "Total gradient accumulation steps:  128\n",
            "Total tokens processed:  0\n",
            "Micro Batch :  50\n",
            "Step :  0 / 20000\n",
            "Total batches:  343\n",
            "Total gradient accumulation steps:  128\n",
            "Total tokens processed:  0\n",
            "Micro Batch :  60\n",
            "Step :  0 / 20000\n",
            "Total batches:  343\n",
            "Total gradient accumulation steps:  128\n",
            "Total tokens processed:  0\n",
            "Micro Batch :  70\n",
            "Step :  0 / 20000\n",
            "Total batches:  343\n",
            "Total gradient accumulation steps:  128\n",
            "Total tokens processed:  0\n",
            "Micro Batch :  80\n",
            "Step :  0 / 20000\n",
            "Total batches:  343\n",
            "Total gradient accumulation steps:  128\n",
            "Total tokens processed:  0\n",
            "Micro Batch :  90\n",
            "Step :  0 / 20000\n",
            "Total batches:  343\n",
            "Total gradient accumulation steps:  128\n",
            "Total tokens processed:  0\n",
            "Micro Batch :  100\n",
            "Step :  0 / 20000\n",
            "Total batches:  343\n",
            "Total gradient accumulation steps:  128\n",
            "Total tokens processed:  0\n",
            "Micro Batch :  110\n",
            "Step :  0 / 20000\n",
            "Total batches:  343\n",
            "Total gradient accumulation steps:  128\n",
            "Total tokens processed:  0\n",
            "Micro Batch :  120\n",
            "Step :  0 / 20000\n",
            "Total batches:  343\n",
            "Total gradient accumulation steps:  128\n",
            "Total tokens processed:  0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 1/20000 [03:33<1184:04:50, 213.15s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(11.0315, device='cuda:0')\n",
            "Step :  1 / 20000\n",
            "Total batches:  343\n",
            "Total gradient accumulation steps:  128\n",
            "Micro Batch :  0\n",
            "Step :  1 / 20000\n",
            "Total batches:  343\n",
            "Total gradient accumulation steps:  128\n",
            "Total tokens processed:  0\n",
            "Micro Batch :  10\n",
            "Step :  1 / 20000\n",
            "Total batches:  343\n",
            "Total gradient accumulation steps:  128\n",
            "Total tokens processed:  0\n",
            "Micro Batch :  20\n",
            "Step :  1 / 20000\n",
            "Total batches:  343\n",
            "Total gradient accumulation steps:  128\n",
            "Total tokens processed:  0\n",
            "Micro Batch :  30\n",
            "Step :  1 / 20000\n",
            "Total batches:  343\n",
            "Total gradient accumulation steps:  128\n",
            "Total tokens processed:  0\n",
            "Micro Batch :  40\n",
            "Step :  1 / 20000\n",
            "Total batches:  343\n",
            "Total gradient accumulation steps:  128\n",
            "Total tokens processed:  0\n",
            "Micro Batch :  50\n",
            "Step :  1 / 20000\n",
            "Total batches:  343\n",
            "Total gradient accumulation steps:  128\n",
            "Total tokens processed:  0\n",
            "Micro Batch :  60\n",
            "Step :  1 / 20000\n",
            "Total batches:  343\n",
            "Total gradient accumulation steps:  128\n",
            "Total tokens processed:  0\n",
            "Micro Batch :  70\n",
            "Step :  1 / 20000\n",
            "Total batches:  343\n",
            "Total gradient accumulation steps:  128\n",
            "Total tokens processed:  0\n",
            "Micro Batch :  80\n",
            "Step :  1 / 20000\n",
            "Total batches:  343\n",
            "Total gradient accumulation steps:  128\n",
            "Total tokens processed:  0\n",
            "Micro Batch :  90\n",
            "Step :  1 / 20000\n",
            "Total batches:  343\n",
            "Total gradient accumulation steps:  128\n",
            "Total tokens processed:  0\n",
            "Micro Batch :  100\n",
            "Step :  1 / 20000\n",
            "Total batches:  343\n",
            "Total gradient accumulation steps:  128\n",
            "Total tokens processed:  0\n",
            "Micro Batch :  110\n",
            "Step :  1 / 20000\n",
            "Total batches:  343\n",
            "Total gradient accumulation steps:  128\n",
            "Total tokens processed:  0\n",
            "Micro Batch :  120\n",
            "Step :  1 / 20000\n",
            "Total batches:  343\n",
            "Total gradient accumulation steps:  128\n",
            "Total tokens processed:  0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 2/20000 [07:04<1177:06:20, 211.90s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(11.0287, device='cuda:0')\n",
            "Step :  2 / 20000\n",
            "Total batches:  343\n",
            "Total gradient accumulation steps:  128\n",
            "Micro Batch :  0\n",
            "Step :  2 / 20000\n",
            "Total batches:  343\n",
            "Total gradient accumulation steps:  128\n",
            "Total tokens processed:  0\n",
            "Micro Batch :  10\n",
            "Step :  2 / 20000\n",
            "Total batches:  343\n",
            "Total gradient accumulation steps:  128\n",
            "Total tokens processed:  0\n",
            "Micro Batch :  20\n",
            "Step :  2 / 20000\n",
            "Total batches:  343\n",
            "Total gradient accumulation steps:  128\n",
            "Total tokens processed:  0\n",
            "Micro Batch :  30\n",
            "Step :  2 / 20000\n",
            "Total batches:  343\n",
            "Total gradient accumulation steps:  128\n",
            "Total tokens processed:  0\n",
            "Micro Batch :  40\n",
            "Step :  2 / 20000\n",
            "Total batches:  343\n",
            "Total gradient accumulation steps:  128\n",
            "Total tokens processed:  0\n",
            "Micro Batch :  50\n",
            "Step :  2 / 20000\n",
            "Total batches:  343\n",
            "Total gradient accumulation steps:  128\n",
            "Total tokens processed:  0\n",
            "Micro Batch :  60\n",
            "Step :  2 / 20000\n",
            "Total batches:  343\n",
            "Total gradient accumulation steps:  128\n",
            "Total tokens processed:  0\n",
            "Micro Batch :  70\n",
            "Step :  2 / 20000\n",
            "Total batches:  343\n",
            "Total gradient accumulation steps:  128\n",
            "Total tokens processed:  0\n",
            "Micro Batch :  80\n",
            "Step :  2 / 20000\n",
            "Total batches:  343\n",
            "Total gradient accumulation steps:  128\n",
            "Total tokens processed:  0\n",
            "Micro Batch :  90\n",
            "Step :  2 / 20000\n",
            "Total batches:  343\n",
            "Total gradient accumulation steps:  128\n",
            "Total tokens processed:  0\n",
            "Micro Batch :  100\n",
            "Step :  2 / 20000\n",
            "Total batches:  343\n",
            "Total gradient accumulation steps:  128\n",
            "Total tokens processed:  0\n",
            "Micro Batch :  110\n",
            "Step :  2 / 20000\n",
            "Total batches:  343\n",
            "Total gradient accumulation steps:  128\n",
            "Total tokens processed:  0\n",
            "Micro Batch :  120\n",
            "Step :  2 / 20000\n",
            "Total batches:  343\n",
            "Total gradient accumulation steps:  128\n",
            "Total tokens processed:  0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 3/20000 [10:36<1178:52:47, 212.23s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(11.0208, device='cuda:0')\n",
            "Step :  3 / 20000\n",
            "Total batches:  343\n",
            "Total gradient accumulation steps:  128\n",
            "Micro Batch :  0\n",
            "Step :  3 / 20000\n",
            "Total batches:  343\n",
            "Total gradient accumulation steps:  128\n",
            "Total tokens processed:  0\n",
            "Micro Batch :  10\n",
            "Step :  3 / 20000\n",
            "Total batches:  343\n",
            "Total gradient accumulation steps:  128\n",
            "Total tokens processed:  0\n",
            "Micro Batch :  20\n",
            "Step :  3 / 20000\n",
            "Total batches:  343\n",
            "Total gradient accumulation steps:  128\n",
            "Total tokens processed:  0\n",
            "Micro Batch :  30\n",
            "Step :  3 / 20000\n",
            "Total batches:  343\n",
            "Total gradient accumulation steps:  128\n",
            "Total tokens processed:  0\n",
            "Micro Batch :  40\n",
            "Step :  3 / 20000\n",
            "Total batches:  343\n",
            "Total gradient accumulation steps:  128\n",
            "Total tokens processed:  0\n",
            "Micro Batch :  50\n",
            "Step :  3 / 20000\n",
            "Total batches:  343\n",
            "Total gradient accumulation steps:  128\n",
            "Total tokens processed:  0\n"
          ]
        }
      ],
      "source": [
        "model.train()\n",
        "train_losses =  torch.zeros(len(train_dataloader))\n",
        "val_losses = torch.zeros(len(val_dataloader))\n",
        "wandb.init(\n",
        "    project='Whisper-From-Scratch'\n",
        ")\n",
        "step = 0\n",
        "for step in tqdm(range(total_iters)):\n",
        "        # print(\"Dataloader things: \", batch)\n",
        "        # print(\"Total batches: \", len(train_dataloader))\n",
        "\n",
        "\n",
        "        # if(device == 0):\n",
        "            # if(step % 100 == 0):\n",
        "        #     if(step == train_loader_length):\n",
        "        #       break\n",
        "        print(\"Step : \", step, \"/\", total_iters)\n",
        "        print('Total batches: ', len(train_dataloader))\n",
        "        print(\"Total gradient accumulation steps: \", gradient_accumulation_steps)\n",
        "                # print(\"Total tokens processed: \", token_count)\n",
        "\n",
        "        # all_gpus_avg_train_loss = None\n",
        "        # all_gpus_avg_val_loss = None\n",
        "        # every once in a while evaluate the loss on train and val sets\n",
        "        if (step  % eval_iters == 0 and step != 0) or step == total_iters - 1:\n",
        "            losses = estimate_loss( val_loader, val_data_iterator, 'cuda')\n",
        "            # avg_train_loss = losses['train']\n",
        "            avg_val_loss = losses['val']\n",
        "            # print(f\"step {step}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "            # if device == 0:  # Only print on main process\n",
        "            print(f\"[GPU {device}] | Step: {step} / {total_iters} | Val Loss: {losses['val']:.4f}\")\n",
        "            # print(f\"[GPU {device}] | Epoch {epoch}/{ModelArgs.epochs}| |Step: {step} | Train Loss: {losses['train']:.4f}\")\n",
        "                # print(f\"step {step}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "                # Log training loss more frequently\n",
        "                # Aggregate average loss across all GPUs\n",
        "            # avg_train_loss = torch.Tensor([losses['train']]).to(device)\n",
        "            avg_val_loss = torch.Tensor([losses['val']]).to(device)\n",
        "            # torch.distributed.reduce(avg_train_loss, dst=0, op=torch.distributed.ReduceOp.SUM)\n",
        "            # torch.distributed.reduce(avg_val_loss, dst=0, op=torch.distributed.ReduceOp.SUM)\n",
        "\n",
        "            # if device == 0:\n",
        "                # all_gpus_avg_train_loss = avg_train_loss / world_size\n",
        "                # print(f\"All_GPUs_Train_losses: {all_gpus_avg_train_loss.item():.4f}\")\n",
        "            all_gpus_avg_val_loss = avg_val_loss / world_size\n",
        "            print(f\"All_GPUs_Val_losses: {all_gpus_avg_val_loss.item():.4f}\")\n",
        "\n",
        "            # if device == 0:\n",
        "\n",
        "                # writer.add_scalar(\"All_GPUs_Train_losses\", all_gpus_avg_train_loss.item(), global_step=step)\n",
        "                # writer.add_scalar(\"All_GPUs_Val_losses\", all_gpus_avg_val_loss.item(), global_step=step)\n",
        "                # writer.add_scalar(\"training_step_loss\", losses['train'], global_step=step)\n",
        "                # writer.add_scalar(\"val_step_loss\", losses['val'], global_step=step)\n",
        "                # writer.add_scalar(\"GPU\", device, global_step=step)\n",
        "                # writer.add_scalar(\"Epoch\", epoch, global_step=step)\n",
        "\n",
        "            wandb.log({\n",
        "                    # \"Learning Rate\": optimizer.param_groups[0]['lr'],\n",
        "                    # \"All_GPUs_Train_losses\": all_gpus_avg_train_loss,\n",
        "                    \"All_GPUs_Val_losses\": all_gpus_avg_val_loss,\n",
        "                    # \"training_step_loss\": losses['train'],\n",
        "                    \"val_step_loss\": losses['val'],\n",
        "                    # \"Step\": step,\n",
        "                    # \"Epoch\": epoch\n",
        "                })\n",
        "\n",
        "\n",
        "\n",
        "        #Loading a checkpoint\n",
        "        # if(os.path.exists('snapshot.pt')):\n",
        "        #    model, optimizer =  _load_snapshot(model=model, optimizer=optimizer, epoch=epoch, step=step, snapshot_path='snapshot.pt')\n",
        "\n",
        "        # if(step % save_chechpoint_iter == 0 and device == 0 and step != 0):\n",
        "\n",
        "        #     _save_snapshot(epoch=epoch, model=model, optimizer=optimizer, step=step)\n",
        "\n",
        "        if step % save_chechpoint_iter == 0 and device == 0 and step != 0:\n",
        "            print(f\"Saving the model checkpoint for step: {step}\")\n",
        "            _save_snapshot(model, optimizer, None, None, step)\n",
        "\n",
        "        accumulated_loss = 0.0\n",
        "\n",
        "\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        for micro_step in range(gradient_accumulation_steps):\n",
        "            try:\n",
        "                spec, idx, y = next(train_data_iterator)\n",
        "            except StopIteration:\n",
        "                train_data_iterator = iter(train_dataloader)\n",
        "                spec, idx, y = next(train_data_iterator)\n",
        "            spec = spec.to(device)\n",
        "            y = y.to(device)\n",
        "            idx = idx.to(device)\n",
        "\n",
        "            # tgt_mask = torch.randint(1, tgt_vocab_size, (batch_size, block_size)).to(device)  #\n",
        "            # print(batch)\n",
        "            # batch = next(train_data_iterator)\n",
        "            # print(batch)\n",
        "            # batch = {k: v.to(self.local_rank) for k, v in batch.items()}\n",
        "            # idx = batch['input_ids'].to(device)\n",
        "            # idx, targets = get_batch(split='train')\n",
        "            # print(f\"Starting the train step: {step}...\")\n",
        "            # for idx, targets in train_loader:\n",
        "            # idx, targets = next(iter(train_loader))\n",
        "\n",
        "            # print(\"Idx: \", idx)\n",
        "            # print(\"Targets: \", targets)\n",
        "\n",
        "            # idx = idx.to(device)\n",
        "            # print(\"Idx: \", idx)\n",
        "            # print(\"Targets: \", targets)\n",
        "            # targets = batch['labels'].to(device)\n",
        "            # token_count += len(idx)\n",
        "            with torch.autocast(device_type=device, dtype=torch.float16):\n",
        "                logits = model(spec, idx)\n",
        "                batch_size, block_size, embeddings_dims = logits.shape\n",
        "                # print(logits.shape)\n",
        "                # print(targets)\n",
        "                logits = logits.view(batch_size*block_size, embeddings_dims)\n",
        "                # print(\"OK\")\n",
        "                targets = y.view(batch_size * block_size)\n",
        "                # print(\"OK2\")\n",
        "                loss = nn.functional.cross_entropy(logits, targets, ignore_index=tokenizer.pad_token_id)\n",
        "\n",
        "                loss = loss / gradient_accumulation_steps #IDK why div is done here specifically? Maybe think of it in terms of a very big batch being processed and there is need for equal important of each mini batch for the overall big batch\n",
        "                accumulated_loss += loss.detach()\n",
        "\n",
        "            model.require_backward_grad_sync = (micro_step == gradient_accumulation_steps - 1) # so that we dont synchronize the gradient everytime across the GPU devices\n",
        "            scaler.scale(loss).backward()\n",
        "            # print(\"loss: \", loss.item())\n",
        "                # Check for unused parameters\n",
        "            unused_params = find_unused_parameters(model)\n",
        "            if unused_params:\n",
        "                print(f\"Unused parameters: {unused_params}\")\n",
        "        # break\n",
        "\n",
        "            # if(device == 0):\n",
        "            if(micro_step % 10 == 0):\n",
        "            #     if(step == train_loader_length):\n",
        "            #       break\n",
        "\n",
        "                    print(\"Micro Batch : \", micro_step)\n",
        "                    print(\"Step : \", step, \"/\", total_iters)\n",
        "                    print('Total batches: ', len(train_dataloader))\n",
        "                    print(\"Total gradient accumulation steps: \", gradient_accumulation_steps)\n",
        "                    print(\"Total tokens processed: \", token_count)\n",
        "            # count += 1\n",
        "\n",
        "        lr = get_lr(step)\n",
        "        for params in optimizer.param_groups:\n",
        "            params['lr'] = lr\n",
        "\n",
        "\n",
        "\n",
        "        # Compute gradient norms before clipping\n",
        "        if(clip != 0.0):\n",
        "            scaler.unscale_(optimizer) #To avoid underflow\n",
        "            total_norm_before = torch.norm(\n",
        "                torch.stack([torch.norm(p.grad.detach(), 2) for p in model.parameters()]), 2\n",
        "            )\n",
        "\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=clip)\n",
        "\n",
        "            # Compute gradient norms after clipping\n",
        "            total_norm_after = torch.norm(\n",
        "                torch.stack([torch.norm(p.grad.detach(), 2) for p in model.parameters()]), 2\n",
        "            )\n",
        "\n",
        "            if(device  == 0 and step !=0):\n",
        "                print(f\"Gradient Norm Before Clipping: {total_norm_before.item():.4f}\")\n",
        "                print(f\"Gradient Norm After Clipping: {total_norm_after.item():.4f}\")\n",
        "\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        # optimizer.step()\n",
        "        # new_scheduler.step()\n",
        "        print(accumulated_loss)\n",
        "        # torch.cuda.synchronize()\n",
        "        # torch.distributed.reduce(loss, dst=0, op=torch.distributed.ReduceOp.SUM)\n",
        "        # if(device == 0):\n",
        "        wandb.log({\n",
        "                    \"Learning Rate\": lr,\n",
        "                    \"All_GPUs_Train_losses\": accumulated_loss.item(),\n",
        "                    # \"All_GPUs_Val_losses\": all_gpus_avg_val_loss,\n",
        "                    # \"training_step_loss\": losses['train'],\n",
        "                    # \"val_step_loss\": losses['val'],\n",
        "                    \"Step\": step,\n",
        "                    # \"Epoch\": epoch\n",
        "\n",
        "                })\n",
        "\n",
        "\n",
        "        # model.train()\n",
        "        # wandb.log({\n",
        "        #   \"Train Loss\": train_losses.mean(),\n",
        "        #   \"Val Loss\": val_losses.mean(),\n",
        "        #   # \"epoch\": epoch\n",
        "        # })\n",
        "        # print(\"Epoch: \", epoch, \"|\", \"Train Loss: \", train_losses.mean(),  \"|\", \"Val Loss: \", val_losses.mean())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "92c4ae77-c006-43e9-a677-343c07e75e87",
        "_uuid": "8cf1c23a-fb45-45af-a00e-3219a7a750c8",
        "jupyter": {
          "outputs_hidden": false
        },
        "trusted": true,
        "id": "UmELOJIEpsOt"
      },
      "outputs": [],
      "source": [
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "e23d9c5c-bfe8-4e86-8fd7-da9eb29e2901",
        "_uuid": "83764454-8200-4a10-9ab4-322effc15cb2",
        "jupyter": {
          "outputs_hidden": false
        },
        "trusted": true,
        "id": "_X9rlI-ApsOt"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true,
        "id": "0JTSt1DgpsOt"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true,
        "id": "ux7EkLwtpsOt"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true,
        "id": "7FLisl7UpsOt"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true,
        "id": "CjYbJOoxpsOt"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true,
        "id": "bzk_j9m_psOt"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true,
        "id": "7nAHEP1rpsOu"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true,
        "id": "Tc8pcHgmpsOu"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true,
        "id": "cenj1I7dpsOu"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true,
        "id": "1DRS6XCJpsOu"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true,
        "id": "NNy8RUV9psOu"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [],
      "dockerImageVersionId": 30919,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "f582fde0045543b9826c2a5808e1836c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ac1fd3699fe84a47aedb002bc285f094",
              "IPY_MODEL_4a238f8102e54ea6bde07382ead69adf",
              "IPY_MODEL_54b5753483a04ace97454b852d422686"
            ],
            "layout": "IPY_MODEL_e9506522e1a447bbaca31e38ec791f05"
          }
        },
        "ac1fd3699fe84a47aedb002bc285f094": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6fd54e53840045a497406dcafdb386b0",
            "placeholder": "​",
            "style": "IPY_MODEL_e2cc4cb317334388ab6f4fb0b17bcbe3",
            "value": "Flattening the indices: 100%"
          }
        },
        "4a238f8102e54ea6bde07382ead69adf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_561c0da5a1184145995c8746ca39f565",
            "max": 10480,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6e148d7d730340b5b5e1dcac1c843237",
            "value": 10480
          }
        },
        "54b5753483a04ace97454b852d422686": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_443e9f22bd2b448a8e269e63983f13d9",
            "placeholder": "​",
            "style": "IPY_MODEL_00d8358718b34a3e949a916d553295db",
            "value": " 10480/10480 [00:01&lt;00:00, 9909.49 examples/s]"
          }
        },
        "e9506522e1a447bbaca31e38ec791f05": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6fd54e53840045a497406dcafdb386b0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e2cc4cb317334388ab6f4fb0b17bcbe3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "561c0da5a1184145995c8746ca39f565": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6e148d7d730340b5b5e1dcac1c843237": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "443e9f22bd2b448a8e269e63983f13d9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "00d8358718b34a3e949a916d553295db": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "123dc6f1d5234259a441a65ec8051463": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_add9244242d3438bb0c55999c7f398bd",
              "IPY_MODEL_ca83532cd81e4d319d4db258f939f0d8",
              "IPY_MODEL_51a35a70be0d480a816828b682f40589"
            ],
            "layout": "IPY_MODEL_aa5e87f2aa3844b597da58e2a0f24e81"
          }
        },
        "add9244242d3438bb0c55999c7f398bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_004db210db4d456890ecfdf317ef02f5",
            "placeholder": "​",
            "style": "IPY_MODEL_9fbe078526744495bdc1324948e7a0ce",
            "value": "Filter: 100%"
          }
        },
        "ca83532cd81e4d319d4db258f939f0d8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_250c4728407044faa0f17204d854ebf7",
            "max": 10480,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e53ffc1619a046e7ae08393f4a6b41c2",
            "value": 10480
          }
        },
        "51a35a70be0d480a816828b682f40589": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d3dfa012744c46fbb325e98d343e937d",
            "placeholder": "​",
            "style": "IPY_MODEL_331478bcce864bf79dd0536027508930",
            "value": " 10480/10480 [00:00&lt;00:00, 213826.46 examples/s]"
          }
        },
        "aa5e87f2aa3844b597da58e2a0f24e81": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "004db210db4d456890ecfdf317ef02f5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9fbe078526744495bdc1324948e7a0ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "250c4728407044faa0f17204d854ebf7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e53ffc1619a046e7ae08393f4a6b41c2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d3dfa012744c46fbb325e98d343e937d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "331478bcce864bf79dd0536027508930": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1922829153554bcaa5ce5a6800c2d275": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0ea10580caa04c949349ec5009d48b30",
              "IPY_MODEL_927489fc0f5a4513ad02856f695713dd",
              "IPY_MODEL_e70a1a9c1c0442e4bd942cfa34af30e4"
            ],
            "layout": "IPY_MODEL_48651b3ffd73444791f8133e2e26220c"
          }
        },
        "0ea10580caa04c949349ec5009d48b30": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_74adcac4afce41fdb3942f851ca6d76d",
            "placeholder": "​",
            "style": "IPY_MODEL_1f09efeed3634f62918cd0494ce57e01",
            "value": "Flattening the indices: 100%"
          }
        },
        "927489fc0f5a4513ad02856f695713dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cf538f36c9b149d3b2bacd43a920aae2",
            "max": 2620,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_dd100a076a5f4f2baa9cbd2db375d49e",
            "value": 2620
          }
        },
        "e70a1a9c1c0442e4bd942cfa34af30e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e67ba48851b04f7ea8796732c54fffb2",
            "placeholder": "​",
            "style": "IPY_MODEL_9dacc8519823484eaa91f7cbd1bc4f1a",
            "value": " 2620/2620 [00:00&lt;00:00, 14379.59 examples/s]"
          }
        },
        "48651b3ffd73444791f8133e2e26220c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "74adcac4afce41fdb3942f851ca6d76d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1f09efeed3634f62918cd0494ce57e01": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cf538f36c9b149d3b2bacd43a920aae2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dd100a076a5f4f2baa9cbd2db375d49e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e67ba48851b04f7ea8796732c54fffb2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9dacc8519823484eaa91f7cbd1bc4f1a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c7b0af32d3e04560a8f0599ab80cd03b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4a3c91f68a4144e3874a847532edcd97",
              "IPY_MODEL_8444e340e21142da8feb5e494f9e3574",
              "IPY_MODEL_72094db159df435397f941d6ed6f8f99"
            ],
            "layout": "IPY_MODEL_e36e1c1b23bf46d7b027048f2f5a2d59"
          }
        },
        "4a3c91f68a4144e3874a847532edcd97": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_952c5a6d7d084337b73d94ce620672cd",
            "placeholder": "​",
            "style": "IPY_MODEL_d311901336cb461cadca3ae7bb04537c",
            "value": "Filter: 100%"
          }
        },
        "8444e340e21142da8feb5e494f9e3574": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_adfcda976ab54a8bace5657809a2451d",
            "max": 2620,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d65629f2014049f3a063d611f7994cc7",
            "value": 2620
          }
        },
        "72094db159df435397f941d6ed6f8f99": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0ceeb78ae5154fd58f1a2dd7d17d072b",
            "placeholder": "​",
            "style": "IPY_MODEL_be9fc651c297414fb4e2a3317ea89f8f",
            "value": " 2620/2620 [00:00&lt;00:00, 77845.07 examples/s]"
          }
        },
        "e36e1c1b23bf46d7b027048f2f5a2d59": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "952c5a6d7d084337b73d94ce620672cd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d311901336cb461cadca3ae7bb04537c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "adfcda976ab54a8bace5657809a2451d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d65629f2014049f3a063d611f7994cc7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0ceeb78ae5154fd58f1a2dd7d17d072b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "be9fc651c297414fb4e2a3317ea89f8f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}